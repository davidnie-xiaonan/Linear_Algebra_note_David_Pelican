var tipuesearch = {"pages":[{"title":"1-1 About this note","text":"A Welcome from David Hello everybody! I am very happy that you chose to go through my note. Linear algebra is a fantastic branch of math and no need to say, most of the students don't understand what is going on even after the final exam of freshman yea's linear algebra. I want this note to be useful to you if you are a grade 12 student, and you are willing to challenge yourself. Or you could be a first-year university student struggling in this course and you want to pass the course. Or you could be a second or third-year STEM student and already learned all the concepts but willing to get a better understanding of how linear algebra applies in your field. Finally, I also want this note to be useful even if you are a TA or a high school teacher who teach relevant concepts. so you might find some new perspectives or interesting examples that you can apply in your own teaching. What can you get from this note First of all, Linear algebra meant to be a university-level course, so you should understand is that the course itself is not easy. Also, since Linear algebra has impacts on so many fields such as statistics, economy, physics, computer science, this note will touch a lot of different concepts/examples from other fields. Nevertheless, this is a \"note\" of linear algebra, one should not expect to only use this as the reference. However, this note can give you an easier, more practical approach to linear algebra. Instead of introducing all the different concepts from nothing and force students to memorize the detail, we are trying something different here. Every chapter will begin with motivation and each concept will be explained not by proof. To see the proof, you can go through the textbook or search online. Here, we only focus on the true purpose of introducing the new idea and how is this new idea helps us in different ways. Of course, practice makes perfect, so each chapter will be followed by a relatively big problem set. Each of the questions in the problem set will have a simple explanation. Hopefully, everyone can get a much better understanding after reading the note and be able to perform some useful calculations. In the end, I would like to quote what Dieudonné said in his \"Foundations of Modern Analysis, Vol. 1\": There is hardly any theory which is more elementary (than linear algebra), in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices.","tags":"Chapter 1","url":"1-1-about-this-note","loc":"1-1-about-this-note"},{"title":"1-2 prereq","text":"One of the fascinate thing about Linear Algebra is that the prerequisites of this course is nearly nothing. When I first learn the linear algebra, I was amazed by the fact that you can do so many thing with just some simple integer number operation. But to catch the essence of linear algebra, we need to review some basic algebra that everybody do in grade 8: The linear equation. We all learned the linear equations, the simple algebraic expression that somehow describe the shape of a line in Cartesian plane. The algebra expression: $$y=mx+b$$ We call this slope-intercept form. We can also rearrange the equation and write down the same thing with a expression looks like: $$ax+by=c$$ both form are telling the same relation between variable \\(x\\) and variable \\(y\\) and if we draw all the pair of \\(x\\) and \\(y\\) as a coordinate in Cartesian plane, we will get a 2d line. all the \\(a\\) , \\(b\\) , \\(c\\) , \\(m\\) , \\(b\\) are just some numbers. for example: $$5x+2y =3$$ Now lets do some algebra. Say we have two lines, let's call them line A and line B. the top equation is the equation for line A and line B is: $$2x+y = 1$$ Without graphing them out, how would you find the intercept of two lines? The question itself is fairly easy, we can use substitution or elimination to find the specific \\(x\\) and \\(y\\) value that both satisfy the equation for line A and line B, and that is the coordinate of the intercept. Let's start with substitution, we first take the second equation and move \\(2x\\) to the other side: $$y = 1-{\\color{red}{2x}} $$ then we can substitute \\(y\\) into our first equation: $$5x+ 2{\\color{red}{(1-2x)}} = 3 $$ $$5x+2-4x = 3$$ $$x=1$$ Then we grab the result and replace the \\(x\\) to 1 in either the first or the second equation, we can solve the \\(y\\) : $$y=1-2*{\\color{red}{1}} = -1$$ Now let's do the same question but with elimination. The idea is this: the goal is to solve \\(x\\) and \\(y\\) , but what makes this hard to achieve is the fact that we got two variables in one equation. So, if I can get rid of one variable, we are in a good shape. Can we do that? Sure! Take equation of line B and multiply everything by a factor of 2: $$2x+y = 1 \\Rightarrow 4x+2y = 2$$ Then take the equation for line A and subtract the new equation we got: $$5x+2y - \\color{red}{(4x+2y)} = 3-\\color{red}2$$ Do a little algebra we can see all the \\(y\\) canceled and we left with: $$x=1$$ Repeat what we did in substitution we will end up with the same result. 2. Three dimensional plane Now we moved on to Three dimension. Don't worry if you never study the math in 3d space before, it is not that different from 2d. One major difference is that two variable is not enough for us to describe the location of points in 3d space. Now we need three, and let's call the extra one \\(z\\) . Imagine the 2d Cartesian coordinate is a blanket and you place the blanket on the floor, now \\(x\\) and \\(y\\) describe the position on the blanket and the new \\(z\\) coordinate describe how high the point is. And Ta-da! We can find the coordinate of any point in three dimensional plane! Now we know how to describe a point, some of you might ask, is the preview line equation still works in 3d? Sadly, no. The equation: $$ax+by=c$$ Only gives you the relation between \\(x\\) and \\(y\\) , and there is no restriction on \\(z\\) axis, which means that this equation describe not only the line on the blanket(x-y plane), but also many different lines on top of it or below it. So what it is? The combination of all the lines is a plane. To be more specific, a 2d plane in 3d space. In general, a equation looks like: $$ax+by+cz = d$$ describes a plane in 3d space. But how do we describe a line in 3d space? You can use some math trick, say the intercepts of two plane. Or you can sue what we called vector, which happens to be our first new concept.","tags":"Chapter 1","url":"1-2-prereq","loc":"1-2-prereq"},{"title":"2-1 Vector","text":"What is a vector looks like? Linear algebra begin with a simple concept in physics, more specifically mechanics: the vector. Understanding vector is the first step to understand linear algebra. To do this, we have to visit three different place, since it has different meaning in different field. In physics, vector is any quantity that has both direction and magnitude . For example, energy as a physical quantity is not a vector because it has no specific direction, but velocity is a vector since it has both direction(where are you going) and magnitude(how fast are you going). In computer science, vector as a data structure has a different name called array. An array, is a data structure consisting of a collection of elements (values or variables) . For example, I have many data points like 1,2,3,4,5,6,7, and I want to multiply all of them by 2. In python, we can write this as a numpy array: import numpy as np x = np.array(1,2,3,4,5,6,7) and just multiply them at once: print(2*x) >>> [2,4,6,8,10,12,14] But you might want to ask: What is the definition of a vector in mathematics? Take some guesses, We will reveal the answer at the end of this chapter. In physics, a vector is usually placed in a coordinate system. For example, we can say a 2d vector called x in Cartesian plane to be (1,1), means that the vector is starting from the origin and pointing toward Northeast. Notice that the starting point of a vector is not important, we can freely moved the vector without changing it as long as it maintain the shape. Reason is simple, changing starting point don't change its direction or magnitude. Because of that, we usually place it right on the origin and use the tip coordinate to describe the vector. You can compute the length of this vector using Pythagorean theorem. In our case, the result is: $$ Length = |\\vec{x}| = \\sqrt{1&#94;2+1&#94;2} = \\sqrt{2} $$ I put a little bar on the top of variable \\(\\vec{x}\\) just to clarify that \\(\\vec{x}\\) is a vector. Usually, If you don't do that, we think the \\(x\\) is a Scalar . In physics, a scalar is a quantity that do not have a direction, like a number in math. To write down the vector, we do the same thing that computer science people does: putting two or more numbers together. Once we have the vector written down, we can treat the numbers as coordinate, compute the direction and the length of the vector if we want to do that.","tags":"Chapter 2 Vectors in n dimension","url":"2-1-vector","loc":"2-1-vector"},{"title":"2-2 Adding vector","text":"Vector addition, subtraction and Scalar multiplication Like numbers, you can also add and subtract vectors. But what is the meaning of adding or subtracting two vector, and how do we do that? Imaging you are walking on a 2d plane and you start at the origin. Say there is a penny located at (2,2) and you are about to pick it up. So you walk to the position (2,2). Let us record this motion using vector and write this vector \\(a\\) as: $$ \\vec{a} = (2,2)$$ Then you heard someone yelling at you. The sound is coming from your right. So you walk to this person and return the penny to him unwillingly. Notice that if you look from above, the direction of this motion is Southeast because your moving direction is Northeast at the beginning. For simplicity, let say you walked exactly \\(\\sqrt{2}\\) unit. Now we need to see what is you second motion. As we learned before, moving a vector don't change the vector itself, so if we move this vector and place it on the origin, we can see the tip of the vector land exactly on the point (1,-1). So we do the same thing and record this motion using vector: $$ \\vec{b} = (1,-1)$$ Now we can finally ask ourselves: what is the meaning of adding this two vector \\(\\vec{a}\\) and \\(\\vec{b}\\) ? The answer is quite straightforward. The first motion is \\(\\vec{a}\\) and right after that you did the second motion \\(\\vec{b}\\) . If you combine these two motions and only look at the result of them, it looks like you just walked to the final point where you talked to the guy who thinks you stealed his penny. So how do we calculate the result? It is very obvious to see that the tip of \\(\\vec{b}\\) is sitting at point (3,1). Therefore the result is just: $$\\vec{a}+\\vec{b}= \\vec{c} = (3,1)$$ If you watch carefully, you can find that the x coordinate of the result is just happens to be the result of adding both x coordinate: $$c_x = a_x + b_x = 2+1 = 3$$ Same as y coordinate: $$c_y = a_y + b_y = 2-1 = 1$$ The reason is because we placed everything inside the Cartesian coordinate. If we treat the x coordinate and the y coordinate as vectors sitting exactly on the x and y axis, then every vector can be represent as an addition of those vectors! Once we realized that, we can reverse the addition and break down our \\(\\vec{a}\\) and \\(\\vec{b}\\) . Then we add all the x direction vectors, do the same thing on y direction vectors. We will end up with two sums, one for x direction and one for y direction. So, our job is done! Because the way we write down vector is just to write down their x and y components. $$\\vec{a}+\\vec{b}= (\\color{red}2,\\color{blue}2) + (\\color{red}1,\\color{blue}{-1}) = (\\color{red}3,\\color{blue}1)$$ Next we are going to do some multiplication. Just like number multiplication, the multiplication is just adding the same thing many many times. Luckily, we can also do that in vector. Again, take \\(\\vec{a}\\) from previous section as an example: $$ 2\\times\\vec{a} = 2\\times(\\color{red}2,\\color{red}2) = (2\\times\\color{red}2,2\\times\\color{red}2) = (4,4)$$ We can also multiply a negative scalar like \\(-1\\) . Keep in mind, multiply negative value will reverse the direction of the vector. So far we know how to add any two vectors and multiply any vector with a scalar, it is a good time to introduce the subtraction. To calculate, it is fairly easy. As you expected, instead of adding the coordinate like you did before, this time you just subtract: $$\\vec{a}-\\vec{b} = (2,2)-(1,-1) = (1,3)$$ To visualize it just like we did for addition, we can break down subtraction into addition and multiplication of \\(-1\\) : $$\\vec{a}-\\vec{b} = \\vec{a} + (\\color{red}{-1}\\times\\vec{b})$$ And that is the end of the first chapter! You will see more examples and fun applications in the assignment. The addition and subtraction rules looks easy but it has a huge impact to more important concepts like linear combination, span, and basis. Now it is the time to reveal the answer of the question I gave from the previous chapter: What is a vector in mathematics? Well, mathematics people tend to have a more abstract definition. In short, vector is the object that follows the rules of vector addition and the scalar multiplication. Or you can say that vector is an element of vector space. What is a vector space? Well, we will go back to this topic. For now, you can think of vector space as a realm that has those rules we mentioned above.","tags":"Chapter 2 Vectors in n dimension","url":"2-2-adding-vector","loc":"2-2-adding-vector"},{"title":"2-3 Assignment","text":"1.Two vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) are located at the origin as showing below: (a) Find the vector \\(\\vec{a}-\\vec{b}\\) using the vector subtraction law, and then calculate its magnitude and direction.(you can use angle to describe the direction) (b) Without using the result from (a) and the law, find the magnitude and the direction of \\(\\vec{a}-\\vec{b}\\) (c) compare your result from part(a) and part(b). Ask yourself which one you prefer. 2.You are play chess on a 2d Cartesian plane. Say your knight is located at the corner of the chess board(the origin) and there is a juicy queen sitting at (6,8). Reminder: The knight's movement is unique: it may move two squares vertically and one square horizontally, or two squares horizontally and one square vertically (with both forming the shape of an L). (a) Let's assume your opponent is quite dumb, so he just don't like to move the queen. Now how do you move your knight to kill the queen? (b) We can use the vector notation to explain the movement of knight. For example, if we want to move the knight to (3,3), we can do: $$(3,3) = 1*(2,1) + 1*(1,2)$$ Now, rewrite your result from (a) using vector addition and scalar multiplication. (c) Can you proof that knight can go to any location on the chess board? —- Answer sheet Two vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) are located at the origin as showing below: (a) Find the vector \\(\\vec{a}-\\vec{b}\\) using the vector subtraction law, and then calculate its magnitude and direction.(you can use angle to describe the direction) By using subtraction law, we can easily get: $$\\vec{a}-\\vec{b} = (3,4) - (2,-2) = (1,6)$$ We can find the magnitude using Pythagorean theorem: $$|\\vec{a}-\\vec{b}| = \\sqrt{1&#94;2+6&#94;2} = \\sqrt{37} = 6.082$$ To find the direction, we can use the inverse trig function: $$\\theta = \\arctan(6/1) = \\arctan(6) $$ And that is roughly East 80.5 degree North. (b) Without using the law of vector addition and subtraction, find the magnitude and the direction of \\(\\vec{a}-\\vec{b}\\) . This time no subtraction allowed. One way we can solve this is to use the cosine law. we began with solving the magnitude of both \\(\\vec{a} $ and $\\vec{b}\\) : $$ |\\vec{a}| = \\sqrt{3&#94;2+4&#94;2} = 5$$ $$ |\\vec{b}| = \\sqrt{2&#94;2+-2&#94;2} = 2\\sqrt{2}$$ Then we find the angle between them: $$\\theta = 53&#94;\\circ + 45&#94;\\circ = 98&#94;\\circ $$ then we use cosine law: $$ c&#94;2 = a&#94;2 + b&#94;2 - 2ab\\cos(\\theta)$$ $$ c = \\sqrt{25+8-2\\times5\\times2\\sqrt{2}\\times \\cos({98&#94;\\circ}}) = \\sqrt{36.96} = 6.079$$ To get the angle, we can first use sine law to get the angle for other two angle in the triangle: $$\\angle{A} = \\arcsin(\\frac{\\sin(98&#94;\\circ)\\times5}{6.079})= 54.5&#94;\\circ$$ Then we can calculate the angle between \\(\\vec{a}-\\vec{b}\\) and the horizontal line: $$\\theta = 180&#94;\\circ-45&#94;\\circ-54.5&#94;\\circ = 80.5&#94;\\circ$$ (c) compare your result from part(a) and part(b). Ask yourself which one you prefer. There is no need to tell which one is easier. In practice, if we have chance to put vector in coordinate, we always do that first. You are play chess on a 2d Cartesian plane. Say your knight is located at the corner of the chess board(the origin) and there is a juicy queen sitting at (6,8). Reminder: The knight's movement is unique: it may move two squares vertically and one square horizontally, or two squares horizontally and one square vertically (with both forming the shape of an L). (a) Let's assume your opponent is quite dumb, so he just don't like to move the queen. Now how do you move your knight to kill the queen? There are multiple ways to do this, here I will only show one way: (b) We can use the vector notation to explain the movement of knight. For example, if we want to move the knight to (3,3), we can do: $$(3,3) = 1*(2,1) + 1*(1,2)$$ Now, rewrite your result from (a) using vector addition and scalar multiplication. This is also fairly straightforward, just read the vector from your graph and write them down as a collection of vectors: $$(6,8) = 1*(-2,1) + 3*(2,1)+ 2*(1,2)$$ (c) Can you prove that knight can go to any location on the chess board? To prove the knight can go to any location, we have to design a path for any location (a,b). One way to do this is to find a path for the knight to go to (1,0), then repeat it a times to get to the position (a,0). After that, find a path for the knight to go to (0,1), then repeat it b times to get to the position (a,b). the below diagram shows how to jump to (1,0) and (0,1) respectively. $$(1,0) = (1,2) + (2,-1)+ (-2,-1)$$ $$(0,1) = (2,1) + (-1,2)+ (-1,-2)$$","tags":"Chapter 2 Vectors in n dimension","url":"2-3-assignment","loc":"2-3-assignment"},{"title":"3-1 Linear combination of vectors","text":"From last chapter, we learned how to add and subtract vectors. We also know that how to expand or compress a vector using scalar multiplication. Now we can finally explain what is \"Linear\" in the word: linear algebra. Say we have a set of many 2d vectors. Let's call them: $$\\pmb{v}_1,\\pmb{v}_2,\\pmb{v}_3,\\pmb{v}_4,...,\\pmb{v}_n$$ I have changed the notation a bit, instead of putting a little arrow on the letter, I just bold the symbols. Notice that every single one of them has to be in a 2d Cartesian plane, because we cannot add 2d vector and 3d vector. Then, I pick few of them, randomly expand or compress them by multiplying some numbers. Then add everything up. At the end, I would expect the vector to be another 2d vector, let's call it \\(\\pmb{a}\\) : $$\\pmb{a} = c_1\\pmb{v}_1+c_2\\pmb{v}_2+c_3\\pmb{v}_3+...+c_n\\pmb{v}_n$$ the set of numbers \\(c_1,c_2,c_3,...,c_n\\) tells you how you expand or compress each vector. For example, I want to create a vector that is just half of the vector \\(\\pmb{v}_1\\) , then we can do: $$\\pmb{a} = 0.5\\pmb{v}_1+0\\pmb{v}_2+0\\pmb{v}_3+...+0\\pmb{v}_n$$ If this is how you construct vector \\(\\pmb{a}\\) , we say that \\(\\pmb{a}\\) is a Linear combination of \\(\\pmb{v}_1,\\pmb{v}_2,\\pmb{v}_3,\\pmb{v}_4,...,\\pmb{v}_n\\) . The word \"Linear\" just means you only used the vector addition and scalar multiplication to create your vector. We will see why this is called \"Linear\" later. Spoil alert! it has something to do with how space change after certain operation. Now, what is so great about this linear combination? Well, a lot of ideas are coming from the linear combination, it is just we don't call it that way before. For example, you can get any point(vector) on a line by multiplying a vector. We can say it is just the linear combination of that vector. Similarly, you can get any point(vector) on a plane by \"linear\" combining two vectors. This is exactly how we construct the 2d Cartesian plane, by combining vector (1,0) and (0,1)! Later on, you will see that the above examples will can be explained using the new concepts in linear algebra.","tags":"Chapter 3 Meaning of \"Linear\" in linear algebra","url":"3-1-linear-combination-of-vectors","loc":"3-1-linear-combination-of-vectors"},{"title":"3-2 Span of vectors and space","text":"As we just learned from last section, we can create a new vector by linear combining other vectors. But why do need this tool at all? Let's go back to the beginning. Before we learn any linear algebra, we mentioned the lines in three dimension space. We find that describing lines in 2d Cartesian plane is much easier than do the same thing in 3d space. So can we find a new way to describe lines, planes or even space using linear combination? Of course! But there is one thing we need to consider. The linear combination of vectors will only give you another vector, but we want more than just one vector. For example, if I have a vector in 2d plane: $$ \\pmb{v} = (1,2) $$ if we do a linear combination of only this vector, we can get a new vector by scalar multiplication: $$ \\pmb{v}_{new} = (1,2)\\times2 = (2,4)$$ we can do this for other numbers too, but you will see that no matter how you change the number, you will always end up with the vector on the same line. If you collect all the possible linear combination, you will just get a line. In linear algebra, we have a specific name for such collection of vectors: The span of vector (2,1) Now, let's see what can we get if we span two vector. For example, the span of vector (1,0) and (0,1). You will soon realize that one can easily get any vector in 2d plane by linear combining (1,0) and (0,1): $$(a,b) = a(1,0) + b(0,1)$$ So, span of vector (1,0) and (0,1) will give you the whole 2d plane. You can imaging that if we graph the span of more vectors, we can end up with any multidimensional space. For example, to describe a three dimension space, we can pick three vectors lying in the space: $$\\pmb{v}_1 = (1,0,0)$$ $$\\pmb{v}_2 = (0,1,0)$$ $$\\pmb{v}_3 = (0,0,1)$$ To get any vector lying in the space, we can do a linear combination of the three vector: $$\\pmb{v} = a_1\\pmb{v}_1+a_2\\pmb{v}_2+a_3\\pmb{v}_3$$ What if we span only two vectors in 3d space? Well, just like what happens in 2d plane, most of the time , we will also get a plane in 2d space. But, what if we span two colinear vectors? If we look at the image above, we can see that spanning vector (1,2) and a same direction vector (2,4) will not give you a whole plane. All the linear combinations of these two vector will just sitting on the same line. So, spanning two vectors will not always give you 2d plane, sometime it could be a line, sometime even just a zero vector. Now you might ask, if I want to span two vector to get a plane, or I want to span three vector to get a 3d space, how should I select my vectors? This question is fairly easy to answer in 2d: just don't pick the vectors that sit on the same line. But we need a better answer when we raise the dimension. In short, we want to have all the vector to be \"unique\", means they were not the linear combination of other vectors. Of course, we have another fancy name for this kind of vectors: we call them \"Linear Independent vectors\" . For example: In 2d, There is no way you can linear combine (1,0) to get (0,1), so they are linear independent. But you cannot say that (0,1) and (0,4) are linear independent vectors, because you can write (0,4) to be a linear combination of vector (0,1): $$(0,4) = 4(0,1)$$ If that is the case, we call them \"Linear Dependent vectors\" . In 3d, things are getting a little more complicated, because finding one vector is the linear combination of other two algebraically are not easy. So, we need more useful tools to calculate. However, we can see what happens when we have one vector is the linear combination of other two in 3d space. Let's look at the graph below: The vectors shown above are: $$ \\pmb{v}_1 = (1,1,-2) $$ $$ \\pmb{v}_2 = (-1,2,5) $$ $$ \\pmb{v}_3 = (0,1,1) $$ To proof one vector is the linear combination of other two, we can rewrite \\(\\pmb{v}_1\\) as : $$ \\pmb{v}_1 = 3\\pmb{v}_3-\\pmb{v}_2 $$ Unlike the colinear vector in 2d space, linear independent vector in 3d can be visualize as the vector that sit on the plane spanned by other two vector.","tags":"Chapter 3 Meaning of \"Linear\" in linear algebra","url":"3-2-span-of-vectors-and-space","loc":"3-2-span-of-vectors-and-space"},{"title":"3-3 Basis of space","text":"Finally, we are can talk about the building block of N-dimension space: The basis. As we learned from last section, we can linear combine a set of vectors to form a line, a plane, or even space. Also, we knew that there is a restriction. Combine these two idea, we will have our definition of basis: We called a set of vectors \\(\\pmb{S}\\) is a basis of vector space \\(V\\) if every element in \\(V\\) can be written as a unique linear combination of vectors in \\(\\pmb{S}\\) and all the vectors in \\(\\pmb{S}\\) is linear independent . For example, \\(set[(0,1),(1,0)]\\) is a basis of 2d vector space (real vector space) since any vector in the vector space (a,b) can be written as a unique linear combination of \\((0,1)\\) and \\((1,0)\\) . Not only that, \\((0,1)\\) and \\((1,0)\\) cannot be written as a linear combination of each others, so they are linear independent. If the set spans the vector space but the vectors in it are not linear independent. We can still called them the generating set or the spanning set of vector space. The name comes from the fact that the vectors \"generate\" the whole space. In linear algebra, we can use the basis to represent the corresponding vector space. But what is the actual difference between two different basis if they describe the exact same vector space? We can construct the 2d Cartesian plane use \\(set[(0,1),(1,0)]\\) , we can also do the same thing with \\(set[(1,1),(1,0)]\\) . So what is the point of writing vector space in terms of different basis? Is it just for fun? Answer is of course not. In fact, this idea of presenting vector space in different basis is a huge topic in linear algebra. Change basis of a vector space will simultaneously change all the vectors in that space. To understand how the vector change in space, we have to learn the \"essence\" of linear algebra: the linear transformation.","tags":"Chapter 3 Meaning of \"Linear\" in linear algebra","url":"3-3-basis-of-space","loc":"3-3-basis-of-space"},{"title":"3-4 Assignment","text":"1.Let \\(A\\) , \\(B\\) be two vectors in 2d real vector space \\(\\mathbb{R}_2\\) . (a)If \\(A = (1,2)\\) and \\(B = (1,4)\\) , are they linear independent or not? If they are, can you write (4,4) as a linear combination of \\(A\\) and \\(B\\) ? (b) If \\(A = (x,y)\\) and \\(B = (x,2y)\\) , what \\(x\\) and \\(y\\) value will make \\(A\\) and \\(B\\) linear dependent? 2.Let \\(C\\) , \\(D\\) , and \\(E\\) be three vectors in 3d real vector space \\(\\mathbb{R}_3\\) . (a) Let \\(C=(1,2,3)\\) , \\(D=(1,3,1)\\) , and \\(E=(-1,-1,-5)\\) . Show that set \\([C, D, E]\\) cannot be a basis of \\(\\mathbb{R}_3\\) . (b) Which vector in (a) you should change in order to obtain a basis of \\(\\mathbb{R}_3\\) ? (c) Now, change the vector and prove it is a basis of \\(\\mathbb{R}_3\\) . 3.The polynomial \\(P(x) = a_0 + a_1x + a_2x&#94;2+ ... + a_nx&#94;n\\) . (a) Show that P(x) is a linear combination of some monomials. (b) Can you find a basis for the polynomial vector space? If so, prove that they are indeed a basis for the space. (c) What is the dimention of the polynomial vector space? 4.Orthonormal basis is a special kind of basis. It requires all basis vectors to be orthogonal to each other and has length of 1. (a) Show that \\(set[(1,0,0),(0,1,0),(0,0,1)]\\) is a orthonormal basis for \\(\\mathbb{R}_3\\) (b) Find another orthonormal basis for \\(\\mathbb{R}_3\\) (Try not to use the vectors on the axis) Answer sheet 1.Let \\(A\\) , \\(B\\) be two vectors in 2d real vector space \\(\\mathbb{R}_2\\) . (a)If \\(A = (1,2)\\) and \\(B = (1,4)\\) , are they linear independent or not? If they are, can you write (4,4) as a linear combination of \\(A\\) and \\(B\\) ? (b) If \\(A = (x,y)\\) and \\(B = (x,2y)\\) , what \\(x\\) and \\(y\\) value will make \\(A\\) and \\(B\\) linear dependent?","tags":"Chapter 3 Meaning of \"Linear\" in linear algebra","url":"3-4-assignment","loc":"3-4-assignment"},{"title":"4-1 Linear Transformation","text":"We are finally here! All the idea that we introduced earlier, is for this amazing idea in linear algebra: linear transformation . And this idea will lead us to a powerful tool in linear algebra: the matrix. Let's jump right into the definition of linear transformation, and see what is the relationship between linear transformation and matrix multiplication. The two key words are \"Linear\" and \"Transformation\". We will began with the word transformation. In high school, transformation usually refer as the change from one graph to another graph. In here, the transformation can be understand as a function, a map that take in one vector and returns another one. The reason why we don't call it function is because we are interested in the \"movement\". Just like you can visualize the graph being \"compressed\" or \"stretched\". A linear transformation can be visualized as a vector \"moves\" to another vector. Notice that the transformation is not talking about only one vector being transformed into one vector. It is talking about every single vector that lives in the vector space being transformed into another vector. Like when we talk about the word \"function\", we are not only interested in one x value change to a y value, we are interested in all x and y value in the domain and range. But how about the word \"linear\"? Like what we learned before in \"linear\" combination, the word \"linear\" means this type of transformation follows certain rules: additivity and homogeneity . Additivity means that the transformation preserve addition: $$f(\\pmb{a}+\\pmb{b}) = f(\\pmb{a}) + f(\\pmb{b})$$ homogeneity, or more precisely, the homogeneity of degree 1 means the operation of scalar multiplication is preserved. $$f(c\\pmb{a}) = cf(\\pmb{a})$$ So what is so special about this type of transformation compare to others? If you have any set of vector that forms a line, for example, all the vectors that has tips sitting on the x-axis. After the transformation, the vectors will still sitting on a line. So linear transformation will not \"curve\" the space. Also, notice that these two condition also tells you a important detail, which is the zero vector stay as a zero vector. You can check that using the condition. Let's see a simple example: The transformation we will see is the simplest transformation: the transformation of doing nothing at all: $$f(\\pmb{x}) = \\pmb{x}$$ Is this a linear transformation? Let's check additivity first: $$f(\\pmb{a}+\\pmb{b}) = \\pmb{a} + \\pmb{b} $$ $$f(\\pmb{a}) + f(\\pmb{b}) = \\pmb{a} + \\pmb{b}$$ $$f(\\pmb{a}+\\pmb{b}) = f(\\pmb{a}) + f(\\pmb{b})$$ Obviously, this follows the rule. Now let's check homogeneity: $$f(c\\pmb{a}) =c\\pmb{a} = cf(\\pmb{a})$$ lovely! Both rules are satisfied. We can now say that the transformation of doing nothing is a linear transformation. but that is way too boring, let's do a interesting one together: the transformation of rotating everything by 90 degrees counter counterclockwise. To demonstrate the transformation. I will plot the sample vector and many other vectors during the transformation. To not make the graph too crowded, I will only draw the points where the tip of the vectors landed for other vectors. It seems impossible to show the additivity and homogeneity in this case. Unlike the last time, it is not easy to write down the \\(f(\\pmb{x})\\) . However, when you graph the transformation on the basis vectors of 2d space, things gets a lot easier. Let's look at how the basis vectors change under the transformation. More importantly, ask yourself whether if the basis is still a basis after the transformation. As you can see. the basis \\((1,0)\\) and \\((0,1)\\) changed to \\((0,1)\\) and \\((-1,0)\\) . And it looks like the new basis still works well. so if our vector \\(\\pmb{x}\\) has x component \\(a\\) and y component \\(b\\) : $$\\pmb{x} = a(1,0)+b(0,1)$$ in our new basis, the coefficients \\(a\\) and \\(b\\) shouldn't change: $$\\pmb{x}_{new} = a(0,1)+b(-1,0)$$ Now we have enough information to show the rotation by 90 degrees is a linear transformation. You will show this in the assignment. To see this geometrically, if you connect the points to form a grid. You will see that the lines remain as lines after the transformation. The space didn't \"curve\".","tags":"Chapter 4 Transformation: the origin of matrix","url":"4-1-linear-transformation","loc":"4-1-linear-transformation"},{"title":"4-2 Matrix multiplication with vector","text":"Unfortunately, no one can told you what the Matrix is. You have to see it for yourself. For most of us, the first time when we hear the word \"Matrix\" is from the movie Matrix. So, here I quote one of the character in Matrix as the start of this chapter. To \"see\" what matrix really is, the knowledge of linear transformation is required. Good thing is, we have learned what linear transformation is, so now is time to find a way to describe the transformation numerically. Hopefully, we can all \"see\" it by the end of the section. We knew that the linear transformation is the transformation between vector space, and vector space can be expressed as the span of basis vector. So, to write down the linear transformation, we need to see how the basis vector changes under the transformation. Let's again take the example from the last section. The transformation of rotating 90 degrees counterclockwise. Remember last time we said the transformation act on the basis of space and change the basis to a new one: $$(1,0) \\rightarrow \\color{red}{(0,1)}$$ $$(0,1) \\rightarrow \\color{red}{(-1,0)}$$ Now let's introduce a better way to write down the vector. For a vector lives in 2d, we usually write them from up to down and close it with a square bracket: $$\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix}\\rightarrow\\color{red}{\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}}$$ $$\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}\\rightarrow\\color{red}{\\begin{bmatrix} -1\\\\ 0 \\end{bmatrix}}$$ Then, to describe the transformation, we just need to write down the two basis vectors side by side (you will see real soon why we do this): $$T(\\pmb{x}) = \\color{red}{\\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix}}$$ To see why this notation even make sense, we will put this square thing next to a vector. Like functions, we can input a number and return another one, this transformation eat a vector and spit out another vector. In our case, the input vector is the vector before the transformation and the output is after. $$T(\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix}) = \\color{red}{\\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix}}\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}$$ So how did this calculating work? Let's take a close look at each element. The vector (1,0) has 1 as x-component so the result has 1 portion of the first row(the new x basis). It also has 0 as its y-component so it has 0 portion of the second row(the new y basis). Notice this comes from the fact that the coefficients of linear combination don't change during the transformation. If you have one \\(\\hat{x}\\) and zero \\(\\hat{y}\\) , you remains one \\(\\hat{x}\\) and zero \\(\\hat{y}\\) . The only thing that change is the basis. $$\\color{red}{\\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix}}\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} = 1*\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}+ 0 * \\begin{bmatrix} -1\\\\ 0 \\end{bmatrix} = {\\begin{bmatrix} 0 + 0\\\\ 1 + 0 \\end{bmatrix}} = \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}$$ and as you see in the animation, the vector (1,0) indeed moves to (0,1). So what if we want to transform something harder? let's rotate the vector (a,b): $$T(\\begin{bmatrix} a\\\\ b \\end{bmatrix}) = \\color{red}{\\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix}}\\begin{bmatrix} a\\\\ b \\end{bmatrix} = a*\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}+ b * \\begin{bmatrix} -1\\\\ 0 \\end{bmatrix} = {\\begin{bmatrix} 0 + (-b)\\\\ a + 0 \\end{bmatrix}} = \\begin{bmatrix} -b\\\\ a \\end{bmatrix}$$ We can confirm this result is also true by looking at the animation. As you already guessed it, the square thing is called matrix and this operation we did is called the matrix multiplication. Anyone who learned the matrix multiplication before might find this operation slightly different from the way you learned before, because usually the matrix multiplication was introduced like this: Don't panic, you will soon find out the two ways are exactly the same. But imaging matrix multiplication as linear transformation of vectors is much much more intuitive than pure algebra. Can we figure out more things from the matrix multiplication we defined? First is the dimension of matrix. We can see that our transformation matrix has two rows and two columns, we call this matrix a 2 by 2 matrix or 2x2 matrix . In convention, we write the number of rows first and then the columns. So a 4 by 3 matrix will have 4 rows and 3 columns. But why do our matrix has such shape? Why not 10 by 10 or 20 by 20? The answer can be find from the dimension of the input vector and output vector. the transformation takes in a 2d vector and spit out another 2d vector so the dimension of matrix has to be 2 by 2. If the transformation takes in a 3d vector and returns a 2d vector, then the dimension of this transformation matrix has to be 2 by 3. $$T(\\begin{bmatrix} x\\\\ y\\\\ z \\end{bmatrix}) = \\color{red}{\\begin{bmatrix} a & b & c\\\\ d & e & f \\end{bmatrix}}\\begin{bmatrix} x\\\\ y\\\\ z \\end{bmatrix} = x*\\begin{bmatrix} a\\\\ d \\end{bmatrix}+ y * \\begin{bmatrix} b\\\\ e \\end{bmatrix}+ z * \\begin{bmatrix} c\\\\ f \\end{bmatrix}$$ There is also an interesting question to think about: Can any linear transformation be written as a matrix? If that so, then any matrix with any dimension can be treat as a linear transformation. Here is another thinking question: what if two matrix multiply each other? What is the result looks like? More importantly, what is the meaning of doing that?","tags":"Chapter 4 Transformation: the origin of matrix","url":"4-2-matrix-multiplication-with-vector","loc":"4-2-matrix-multiplication-with-vector"},{"title":"4-3 Assignment","text":"In 4-1, we worked on the transformation that rotation 90 degree counterclockwise. In this practice, you will write down the transformation matrix for any rotation in 2d and prove that it is a linear transformation. Consider a transformation matrix \\(T(\\pmb{x}) R&#94;2\\rightarrow R&#94;2\\) : $$T(\\pmb{x}) = \\begin{bmatrix} 1 & 0\\\\ 0 & 2 \\end{bmatrix}$$ (1) What is this transformation does to the space, you can try to transform some vector to see. (2) What kinds of vectors do not change directions after the transformation? (3) If now the transformation matrix became: $$T(\\pmb{x}) = \\begin{bmatrix} 1 & 1\\\\ 0 & 2 \\end{bmatrix}$$ What kinds of vectors do not change directions after the transformation this time? Do we expect the same answer? Consider following transformation matrix: $$T_1(\\pmb{x}) = \\begin{bmatrix} 2 & 0 & 3\\\\ 0 & 2 & 2 \\end{bmatrix}$$ (1) What is the dimension of vectors before and after the transformation? (2) After such transformation, the space is then transformed by another matrix: $$T_2(\\pmb{x}) = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\\\ 1 & 2 \\end{bmatrix}$$ What is the dimension of vectors after this transformation? (3)Now combine these two transformation into one transformation \\(T_3(\\pmb{x})\\) so that: $$T_3(\\pmb{x}) = T_2(T_1(\\pmb{x}))$$ Can you write \\(T_3(\\pmb{x})\\) down as a matrix? if so, what is the dimension of this matrix \\(T_3(\\pmb{x})\\) ? (hint: Try to think matrix as columns of vector) (4)What if you reverse the order of transformation? Say \\(T_4(\\pmb{x}) = T_1(T_2(\\pmb{x}))\\) ? Does that gives you the same transformation? What is the result tells you? Answer sheet","tags":"Chapter 4 Transformation: the origin of matrix","url":"4-3-assignment","loc":"4-3-assignment"},{"title":"5-1 intercepts problem","text":"Historically, the matrix was not invented for linear transformation, but for a more practical problem: solving linear systems. In this unit, we are going to cover everything you need to know when solving a linear system. Then I will explain why it is better to solve them using matrix. Let's start from the simplest linear system you can imagine: line intercepts. Suppose we have two lines in Cartisian plane, line A and line B: Here are their equations: $$ \\begin{cases} 2x-y\\ =\\ 2 & \\text{Line A}\\\\ x+y\\ =\\ 1 & \\text{Line B} \\end{cases} $$ Now the classic questions are: where does these two lines meet? And do they meet at all? It is fairly easy to see in graph that these two lines do intersect and they intersect at point (1,0). To solve this algebraically is also trivial, you can do a substitution and solve it under a minute. If you can solve this, congratulation! You pass the grade 9 math exam! Clearly, this method is not a good way of solving system of linear equations. Why? I will give you two reasons: First, how do we solve the question with more than 2 dimensions? Well, you might say we introduce more than two variables! a, b, c, d{static}. That is absurd because we only have 26 letters and if we use notation like \\(a_1, a_2\\) , the equation will be not only long, but also nasty to read. Second, how do we know if there is a solution? Assume you are working on this 100 dimension linear system, it took you 3 days to realize that there is no solution! How frustrated is that! So, Let's sum up. We need our new notation to have two things: One, It has to look good, precise, clear, and elegant. Two, it has to have a easy way to check whether there is a solution or not. Luckily, we have matrix. The matrix notation solve the above questions perfectly and even gave us more details about the system. I will show you the way to write system of linear equations using matrix. Let's take the above equations as an example: $$ \\begin{cases} 2x-y\\ =\\ 2 & \\text{Line A}\\\\ x+y\\ =\\ 1 & \\text{Line B} \\end{cases} $$ The left side of the equations are some variable with different coefficients, so let's start from there. Instead of writing all the x and y in our equations, we can put all of them at the side as a vector: $$\\begin{bmatrix} x\\\\ y \\end{bmatrix}$$ then we can write only the coefficients in front of variables as a matrix: $$\\begin{bmatrix} 2&-1\\\\ 1&1 \\end{bmatrix}$$ To get \\(2x-y\\ =\\ 2\\) and \\(x+y\\ =\\ 1\\) back, we can do a matrix multiplication: $$ \\begin{cases} 2x-y\\ \\\\ x+y\\ \\end{cases} \\rightarrow\\begin{bmatrix} 2&-1\\\\ 1&1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix} $$ then we can rewrite the right hand side as a vector too: $$\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}$$ So we ended up with a matrix multiply on an unknown vector equals to a vector: $$ \\begin{bmatrix} 2&-1\\\\ 1&1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix}=\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$ We can write this in a more abstract form: $$\\color{red}{A}\\pmb{x} = \\pmb{b}$$ Where \\(\\color{red}{A}\\) is a matrix, \\(\\pmb{x}\\) is the unknown vector that we are trying to solve and \\(\\pmb{b}\\) is a vector. It is obvious to see that the matrix notation is way easier than the original one since you don't have to write down the variables over and over again. Moreover, it is much faster to do elimination. We will discuss the detail of Gauss elimination in the next section.","tags":"Chapter 5 Solving Linear system using matrix","url":"5-1-intercepts-problem","loc":"5-1-intercepts-problem"},{"title":"5-2 Gaussian elimination","text":"In this section we are going to learn the way to solve linear equations using Gaussian elimination. We are also going to introduced some special form of matrix. So what is Gaussian elimination? In short, it is a algorithm that will Always solve a linear system. If you follow the procedure, no matter how hard the linear systems are, you can solve it guaranteed. Let's go through a simple example: $$ \\begin{cases} 2x-y\\ =\\ 2 &\\color{red}{(1)}\\\\ x+y\\ =\\ 1 &\\color{red}{(2)} \\end{cases} $$ We can write this in a matrix form: $$ \\begin{bmatrix} 2&-1\\\\ 1&1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix}=\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$ To solve this as a high school student, you can try substitution or elimination. Let's use elimination! You will first make a choice of eliminating x or y. It is much easier to eliminate y, all you need is to add first row to the second row and cancel out all the y: $$ \\color{red}{(1)}+ \\color{red}{(2)} \\rightarrow 3x = 3 $$ $$ x = 1 $$ then we substitute the result back to \\((2)\\) : $$1+y = 1$$ $$y = 0 $$ So how to do the same thing in matrix form? The first step we do is to make this linear equation \\(A\\pmb{x} = \\pmb{b}\\) even easier. We will rewrite the equation to something we called Augmented matrix : $$ \\begin{bmatrix} 2&-1\\\\ 1&1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix}=\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$ $$ \\downarrow $$ $$ \\left[ \\begin{matrix} 2 & -1 \\\\ 1 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1 \\end{matrix} \\right. \\right] $$ How do you \"read\" this notation? Well, think about the first column is the coefficients for x and the second column is the coefficients for y and the sum is given by the number after the bar. We will see why this notation suit our purpose in a sec, now if we want to do the same thing as above, then how should we write it? To write \\( \\color{red}{(1)}+ \\color{red}{(2)}\\) , we can change the row \\( \\color{red}{(2)}\\) to our new row \\( \\color{red}{(1)}+ \\color{red}{(2)}\\) : $$ \\color{red}{(2)}\\rightarrow \\color{red}{(1)}+ \\color{red}{(2)}$$ $$ \\left[ \\begin{matrix} 2 & -1 \\\\ 1\\color{red}{+2} & 1\\color{red}{-1} \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1\\color{red}{+2} \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 2 & -1 \\\\ \\color{red}{3} & \\color{red}{0} \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ \\color{red}{3} \\end{matrix} \\right. \\right] $$ this change is so called the row operation , or elementary row operation , There are in total three types of row operations. The first one is row addition , which is exactly what we did above, replacing one row to this row adding another row. then what we did is to divide the new row by 3, in order to get $ x = 1 $. we will perform the same operation here: $$ \\color{red}{(2)}\\rightarrow \\frac{1}{3} \\times \\color{red}{(2)}$$ $$\\left[ \\begin{matrix} 2 & -1 \\\\ 3\\color{red}{\\div3} & 0\\color{red}{\\div3} \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 3\\color{red}{\\div3} \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 2 & -1 \\\\ \\color{red}{1} & \\color{red}{0} \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ \\color{red}{1} \\end{matrix} \\right. \\right] $$ And this is our next type of row operation, called row multiplication . Now comes to the hard part, which is our \"substitution\". In grade 9, these two method: substitution and elimination were taught as different method. But in here, we can see that both method can be break down into numbers of basic row operations. What we did after we figured out the x is we plugged into the original equation \\(\\color{red}{(2)}\\) . But as you see, we don't have the original equations anymore,, so we have to plug into the first one. How to do that? we can first remove all the x from the first equation by doing: $$ \\color{red}{(1)}\\rightarrow \\color{red}{(1)} - 2 \\times \\color{red}{(2)}$$ $$\\left[ \\begin{matrix} 2\\color{red}{-2} & -1 \\\\ 1 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\color{red}{-2} \\\\ 1 \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} \\color{red}{0} & -1 \\\\ 1 & 0 \\end{matrix} \\left| \\, \\begin{matrix} \\color{red}{0} \\\\ 1 \\end{matrix} \\right. \\right] $$ Next we can divide the first row by \\(-1\\) to get $y = $ something instead of $-y = $ something: $$ \\color{red}{(1)}\\rightarrow -1 \\times\\color{red}{(1)} $$ $$\\left[ \\begin{matrix} 0 & -1\\color{red}{\\div-1} \\\\ 1 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 0\\color{red}{\\div-1} \\\\ 1 \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 0 & \\color{red}{1} \\\\ 1 & 0 \\end{matrix} \\left| \\, \\begin{matrix} \\color{red}{0} \\\\ 1 \\end{matrix} \\right. \\right] $$ Basically, our job is done here. However, for simplicity, we would like to show \\(x = 1\\) first and then show \\(y = 0\\) , so we can do the last type of row operation here, called the row switching : $$ \\color{red}{(1)}\\leftrightarrow \\color{red}{(2)} $$ $$\\left[ \\begin{matrix} 0 & 1 \\\\ 1 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 0 \\\\ 1 \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 1 \\\\ 0 \\end{matrix} \\right. \\right] $$ Then, we can easily read the result just by looking at the rows. Alright! We have learned the notation and row operation, now it is the time to find out how to perform the Gaussian elimination. This method is not always the optimum one when you solve linear system, but it is surely the one you can rely on when you have no clue how to start. Let's have a fairly simple example to show how this work. $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 1 & 0 & 2 \\\\ 1 & 3 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1 \\\\ 3 \\\\ \\end{matrix} \\right. \\right] $$ As you can see, we have a system of equations with three unknowns. We started from the first row and we only care about the first number in the row. If this number is not \\(1\\) , we will divide whatever this number is to make our first number to be \\(1\\) . In this case, we have that number already be \\(1\\) so we moved on to next step. The next step is to make the first number on the second row to be \\(0\\) . To do that, you will do a row addition(subtraction). The only row we will use is the first row since the first row already has the coefficient of x to be \\(1\\) , we just need to subtract as many row 1 as we want to get the first number on the second row to be \\(0\\) . In this case, the first number on the second row is 1, so we need to subtract exactly one row 1 to row 2: $$ \\color{red}{(2)}\\rightarrow \\color{red}{(2)}- \\color{red}{(1)} $$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 1 & 0 & 2 \\\\ 1 & 3 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1 \\\\ 3 \\\\ \\end{matrix} \\right. \\right]\\rightarrow \\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 3 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 3 \\\\ \\end{matrix} \\right. \\right] $$ Now we do the similar thing as above, we check if the coefficient of the second number is \\(1\\) or not, if not, we will do a division to make this number to be \\(1\\) . Luckily, this time we get a \\(1\\) again! So we don't have to do anything. You might already see the pattern here. The next step is to make sure that on the third row, we have \\(0\\) for both the first coefficient and the second one. To do that, we subtract different amount of row 1 and row 2. In our case, The first number for the third row is 1, which means that we need to subtract off one row 1: $$ \\color{red}{(3)}\\rightarrow \\color{red}{(3)}- \\color{red}{(1)} $$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 3 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 3 \\\\ \\end{matrix} \\right. \\right]\\rightarrow \\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 4 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 1 \\\\ \\end{matrix} \\right. \\right] $$ Then we subtract out four row 2 since the second number of row 3 is now \\(4\\) . You might find that we can also divide the new row 3 by 4 and we immediately get what y is. But as I mentioned before, the Gaussian elimination is not always the easiest one, so we will follow the more general case. $$ \\color{red}{(3)}\\rightarrow \\color{red}{(3)}- 4\\times\\color{red}{(2)} $$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 4 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 1 \\\\ \\end{matrix} \\right. \\right]\\rightarrow \\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & -4 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 5 \\\\ \\end{matrix} \\right. \\right] $$ we do the same thing again, now this time the leading coefficient is \\(-4\\) , so we divide the row 3 by \\(-4\\) : $$ \\color{red}{(3)}\\rightarrow \\color{red}{(3)}\\div (-4) $$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & -4 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ 5 \\\\ \\end{matrix} \\right. \\right]\\rightarrow \\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ -5/4 \\\\ \\end{matrix} \\right. \\right] $$ Lovely, we are half way there! As you can see, our matrix on the left now became a special shape with all diagonal term to be \\(1\\) and the bottom left to be all zero. This type of matrix is what we called lower triangular matrix , but that is not our topic today so we won't dig on too much. The rest of the job is much easier than the first half, you will do the similar thing but work your way up this time. The goal is to have all diagonal terms to be \\(1\\) and rest to be zero. To do that , we first subtract one row 3 to row 2 to get rid of that annoying \\(1\\) at the end of row 2: $$ \\color{red}{(2)}\\rightarrow \\color{red}{(2)}- \\color{red}{(3)}$$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\\\ -5/4 \\\\ \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1/4 \\\\ -5/4 \\\\ \\end{matrix} \\right. \\right] $$ Then we add one row 2 to row 1 and subtract one row 3 to row 1: $$ \\color{red}{(1)}\\rightarrow \\color{red}{(1)}+\\color{red}{(2)}- \\color{red}{(3)}$$ $$\\left[ \\begin{matrix} 1& -1 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1/4 \\\\ -5/4 \\\\ \\end{matrix} \\right. \\right]\\rightarrow\\left[ \\begin{matrix} 1& 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 7/2 \\\\ 1/4 \\\\ -5/4 \\\\ \\end{matrix} \\right. \\right] $$ And we have our solution. Sadly, you are not always getting a great answer like this. Just like if you are solving two linear equations, you might experience something like two parallel lines or two same lines. When you solve a linear system, a non-trivial solution is not always guaranteed. Sometimes, you might end up with something like this: $$\\left[ \\begin{matrix} 1& 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 0 \\\\ 0 \\\\ 1 \\\\ \\end{matrix} \\right. \\right] $$ Or this: $$\\left[ \\begin{matrix} 1& 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{matrix} \\left| \\, \\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{matrix} \\right. \\right] $$ Don't worry! You will see these cases in the practice. We are not going to discuss these case in detail.","tags":"Chapter 5 Solving Linear system using matrix","url":"5-2-gaussian-elimination","loc":"5-2-gaussian-elimination"},{"title":"5-3  Column space and null space","text":"As we discussed at the beginning of this chapter, We want our new notation to be much better than the old one in two aspects. The first one: Simplicity, is done by writing the equations into matrix. Now comes to the second one, which is to know how many solutions are there before we solve the equations. This sounds great, but how should we do this? Let's start from 2d and then move on to higher dimension. Our linear equations on plane looks like just lines. So solving linear equations means you want to find the certain spot where two of your equations are satisfied simultaneously. In another word, the point (solution) has to be on both lines. So what are the cases? The first case is obvious, two lines intersect at one point. In this case, we have exactly one solution. The second case is two parallel lines. In this case, we have zero solution. The third case is less obvious, you can place your two lines at the same place. In this case, all the points on these lines are the solution. We can say there are infinite solution. I personally don't like to say there are infinite many solution. Here is the reason: Saying infinite solutions is not useful, at least in some cases. For example, there are infinite many points on one line, but there are also infinite many points on two lines, a plane, a little triangle, or a cube. So only saying \"there are infinite many solutions\" won't give you enough information. If we write these three different cases in augment matrix, you will see that the second one and the third one are very special: $$\\left[ \\begin{matrix} 2 & -1 \\\\ 2 & -1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\end{matrix} \\right. \\right] $$ $$\\left[ \\begin{matrix} 2 & -1 \\\\ 2 & -1 \\end{matrix} \\left| \\, \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right. \\right] $$ Both cases has two linearly dependent rows, but the vectors on the right are different. Let's look at them one by one: $$\\left[ \\begin{matrix} 2 & -1 \\\\ 2 & -1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ -1 \\end{matrix} \\right. \\right] $$ This one is saying that \\(2x-y = 2\\) and also \\(2x-y = -1\\) . Obviously this will end up with no solution since you can combine them and give you \\(2 = -1\\) . For this type of linear system, we say they are inconsistent . Then let's look at the next one: $$\\left[ \\begin{matrix} 2 & -1 \\\\ 2 & -1 \\end{matrix} \\left| \\, \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right. \\right] $$ This one is ridiculous. The two equations are literally the same thing! So it didn't tell us enough information about the system! Of course you are getting infinite many solution because you have two variable with only one equation. So Let's sum up here. If the matrix has linear dependent rows, then it means that at least one of the row is \"useless\". So if you have a n by n matrix and one of the row is telling the same thing as other rows, then you will not have sufficient condition to solve this system. Then you will end up with many solutions. If your matrix has linear dependent rows and the vector don't match, then it means you have parallel lines&planes. Then you end up with no solution. Then finally, if all your rows are linear independent, congratulation! You can have a non-trivial solution. But why linear independent rows can give you non-trivial solution? Let's see if we can understand it using the knowledge of linear transformation. The equation we are going to solve is this: $$A\\pmb{\\color{red}x} = \\pmb{b}$$ Solving this equation means to find the vector \\(\\pmb{\\color{red}x}\\) such that after the transformation it became \\(\\pmb{b}\\) . By look at the transformation, we can predict what will happen if we put a vector in there. Let's start with a linear dependent matrix: $$\\begin{bmatrix} 2 &-1\\\\ 2 &-1 \\end{bmatrix}$$ So what is the transformation looks like? As you can see, all the vectors are transformed onto a line \\(y = x\\) . This makes sense since the transformation returns exactly same number for x and y. In fact, any linear dependent 2 by 2 matrix will squish the 2d plane to a line(or a point if everything is zero). So, if we are looking for a specific \\(\\pmb{\\color{red}x}\\) that gives you \\(\\pmb{b}\\) . Then there are two possible results: One is that your \\(\\pmb{b}\\) actually landed on this line so you get some solution. The other case is your \\(\\pmb{b}\\) is unfortunately not on this line so the system is inconsistent, no such \\(\\pmb{\\color{red}x}\\) can give you that \\(\\pmb{b}\\) vector after the transformation. Now why do you have multiple solutions for one \\(\\pmb{b}\\) ? Well, If we just look at the transformation itself, I looks like all the vectors in 2d space are projected on the 1d line. Like the shadow under sun. So it kind of makes sense if different objects returns exactly same shadow. But this is only 2 dimension. What if we have more than 2? what if we have dimension of 10? how do we understand the transformation that happens in 100 dimension? Obviously we need more tools to do that. So, let's use the knowledge of span and basis vector! We know that we can think of any matrices as linear transformation, and each column in a transformation matrix tells us how basis vector transform. So why don't we find the span of those basis vectors after the transformation? That will definitely give us some information about this transformation. Let's take the above matrix as a example: $$\\begin{bmatrix} 2 &-1\\\\ 2 &-1 \\end{bmatrix}$$ The column vectors are: $$\\begin{bmatrix} 2\\\\ 2 \\end{bmatrix},\\begin{bmatrix} -1\\\\ -1 \\end{bmatrix}$$ So the span of those vectors give us the \\(y = x\\) line on 2d plane. What do we know from this? We know that any vectors after this transformation will end up sitting on the line \\(y = x\\) . Why? Well any vector on the plane can be rewrite into a linear combination of the basis vector: $$\\begin{bmatrix} a\\\\ b \\end{bmatrix} = a\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix}+ b\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}$$ Then if we apply the transformation: $$T\\begin{bmatrix} a\\\\ b \\end{bmatrix} = a \\times T\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix}+ b \\times T\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}$$ So the vector after the transformation will be a linear combination of the column vectors \\(T \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} \\) and \\(T \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} \\) . Therefore, the vector after the transformation will for sure sit on the span of column vectors, because the span is the collection of all possible linear combinations. Do that span of column vectors has a name? Yes, it is called Column space . By looking at the column space, we will know where our vectors live after the transformation. It is just like the idea of range in function. Let's look at the dimension of column space. For a n by n matrix, which means you have n variable and n equations(or you can think of a dimension n vector transform to a same dimension vector). We can say the dimension of matrix is n. if its column space also have dimension of n, what does this tell us? Well, it means your vector after the transformation lives in the same dimension n space. In the other word, the columns are linearly independent. Later on we will know that this also means the rows are linearly independent too. Here is the tricky part. What if the dimension is less than n? You might say: Sure! Then it means the linear system has no solution or many solution. But I want to know more about it. An interesting question to ask is: We start with dimension of n, we ended up with dimension of something less than n, so where does this \"missing\" dimension go? Let's look at the equation again: $$A\\pmb{\\color{red}x} = \\pmb{b}$$ Assume we have linear independent columns (or you can say we have only one solution). Let's call the solution \\(\\pmb{x}_1\\) , so: $$A\\pmb{x}_1 = \\pmb{b}$$ Now let's consider the similar equation: $$A\\pmb{x} = \\pmb{0}$$ if there is some non-zero \\(\\pmb{x}\\) satisfy this equation, we are in a big trouble. Why? Assume that \\(A\\pmb{x}_2 = \\pmb{0}\\) for some non-zero \\(\\pmb{x}_2\\) , the I can say there exist a brand new solution for \\(A\\pmb{x} = \\pmb{b}\\) : $$A\\pmb{x}_1 + A\\pmb{x}_2 = \\pmb{b} + \\pmb{0} $$ $$A(\\pmb{x}_1 + \\pmb{x}_2) = \\pmb{b} $$ The new solution is \\(\\pmb{x}_1 + \\pmb{x}_2\\) . For linear independent matrix, this $\\pmb{x}_2 $ has to be zero, other wise you won't get only one solution. What if the rows are linear dependent? Then the equation \\(A\\pmb{x} = \\pmb{0}\\) will have non-zero solution! Take the same matrix as example, if we solve \\(A\\pmb{x} = \\pmb{0}\\) for this matrix we will have: $$\\begin{bmatrix} 2 &-1\\\\ 2 &-1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix}$$ This equation is easy to solve, we just have : $$ 2x-y = 0 $$ All the vectors on the line $ y = 2x $ satisify the equations. We can also write the result as: $$span \\left( \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} \\right) $$ We actually have name for this space, we call it null space or Kernel . \"Null\" means \"nothing\", Of course this does not means there is nothing in the space. It means the collection of all vectors that are transformed to \"nothing\", the zero vector. Let's take a look at what null space is. Here I graphed a vector in 2d and drew some other vectors that are the linear combination of null space vector and the original vector. Notice in our case, all such vector sits on a line since our null space is one dimensional. As you can see, all the vectors were transformed into one same vector as we expected. if we know one solution of the linear system, adding any null space vector to that solution will give you a new solution. So if the null space of your matrix is not zero, then once you have a solution(if the system is consistent), you can get infinite many solution from adding the null space vectors. But if the null space of your matrix is zero, you will just get a unique one. By checking the column space and the null space of a linear system, it is fairly easy to tell if the system has solution or not. Notice that we are no longer focusing on a single linear system. We are now treating the matrix as a linear transformation and focusing on how the whole space change. The linear equations only describes one result of this linear transformation. Once you understand this, it is much easier for you to understand why people are interested in linear algebra.","tags":"Chapter 5 Solving Linear system using matrix","url":"5-3-column-space-and-null-space","loc":"5-3-column-space-and-null-space"},{"title":"5-4  The cheat: inverse matrix","text":"Last time, we focused on the matrix in linear system. By treating the linear equations as transformation of vectors, we can easily predict the numbers of solution. But when we tried to solve the system, we still have to do elimination. So, can we do better? Let's look at our equation in matrix form: $$A\\pmb{\\color{red}x} = \\pmb{b}$$ The thing we are trying to solve is \\(\\pmb{\\color{red}x}\\) , and we know the transformation \\(A\\) and the target vector \\(b\\) . Assume you solved the equation and you have exactly one solution, you know that the null space of this transformation is zero. In another word, the system will always give you a unique \\( \\pmb{\\color{red}x}\\) for any vector \\(\\pmb{b}\\) . But the problem is, when we change the vector \\(\\pmb{b}\\) , we have to solve the system again, and that is not good for us. So, is there a better way to solve this? Let's look at the simplest equation that we all learned in elementary school: $$a\\color{red}x = b$$ Notice that \\(a\\) and \\(b\\) are just number. To solve x, we just divide the \\(a\\) both side. Good thing about this is that no matter how b change, we always divide the number \\(a\\) to get our solution for \\(\\color{red}x\\) . But wait a second, we don't have division in matrix! Obviously you can't divide matrices, it doesn't make any sense! Or is it? The matrix act as a transformation \"function\", like \\( y = sin(x)\\) , you can't just divide functions. But, we can change our \"division\" into the multiplication of the reciprocal! What is the \"reciprocal\" of a function? Well, it is called the inverse of function: $$f&#94;{-1}(f(x)) = x$$ So how do we solve the equation? Let's first see how we did it in function: $$f(x) = b $$ $$f&#94;{-1}(f(x)) = f&#94;{-1}(b) = x $$ $$x = f&#94;{-1}(b) $$ Assume there exist the \"inverse\" of matrix, we can perform the same thing as we did in function: $$A\\pmb{\\color{red}x} = \\pmb{b}$$ $$A&#94;{-1}(A\\pmb{\\color{red}x}) = A&#94;{-1}b = \\color{red}x $$ $$x = A&#94;{-1}(b) $$ So what has to be true for this \"inverse\" in order to make the above idea practical? It is not hard to see that the second line has to be the definition for inverse matrix: $$A&#94;{-1}(A\\pmb{\\color{red}x}) = \\color{red}x $$ If you treat the two matrix \\(A&#94;{-1}A\\) as one matrix, Let's call it \\(I\\) ,what is \\(I\\) transformation looks like? $$A&#94;{-1}A = I$$ $$(A&#94;{-1}A)\\pmb{\\color{red}x} = \\color{red}x $$ $$I\\pmb{\\color{red}x} = \\pmb{\\color{red}x}$$ Well, this matrix has to transform vector \\({\\color{red}x}\\) into itself. Such transformation is called identity matrix . Let's construct the identity matrix together. First, what is the dimension of this matrix? If the vector has to transform to itself, then the matrix must have size of n by n, where n is the dimension of that vector. How about the entries? The basis of this transformation should remain the same: $$\\left( \\begin{bmatrix} 1\\\\ 0\\\\ 0\\\\ \\vdots \\end{bmatrix} , \\begin{bmatrix} 0\\\\ 1\\\\ 0\\\\ \\vdots \\end{bmatrix} , \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ \\vdots \\end{bmatrix} {static}. \\right)$$ For example, a 3 by 3 identity matrix should looks like this: $$\\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix}$$ Now we know what matrix are we looking for, it is time to see how to compute the inverse matrix. Our goal is to find a matrix \\(A&#94;{-1}\\) such that \\(A&#94;{-1}A = I\\) , Here we will introduce one of the methods to compute the inverse. For now, let's accept the fact that this method works. Later on, we will find out why this method works. The method we will use is Gauss–Jordan elimination, a variation of the classic Gaussian elimination. Let's say we want to find the inverse matrix of following matrix A: $$A = \\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix}$$ First, we have to take a look at the matrix, because not all matrices have inverse. There are two things we need to check: First is the size of this matrix, it has to be a square matrix (2 by 2, 3 by 3 etc.). Second, it has to have linear independent rows. Or use the concepts we learned earlier, for a n by n matrix, the dimension of column space has to be n. For this easy example, we can see that both conditions are satisfied. So we know that this matrix has inverse, and now we can compute it. First step is to write a augmented matrix by combining the matrix A and the identity matrix with the same size: $$ [A|I] = \\left[ \\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix} \\right. \\right] $$ Then we do Gaussian elimination. The goal is to make the left part of the augmented matrix to be identity matrix so the right part, which is the identity matrix at the beginning, will became the inverse matrix. Let's get started: $$r_2 \\rightarrow r_2-3r_1$$ $$\\left[ \\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix} \\right. \\right] \\rightarrow \\left[ \\begin{matrix} 1 & 2 \\\\ 0 & -2 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ -3 & 1 \\end{matrix} \\right. \\right] $$ $$r_2 \\rightarrow -\\frac{1}{2}r_2$$ $$\\left[ \\begin{matrix} 1 & 2 \\\\ 0 & -2 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ -3 & 1 \\end{matrix} \\right. \\right] \\rightarrow \\left[ \\begin{matrix} 1 & 2 \\\\ 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{matrix} \\right. \\right] $$ $$r_1 \\rightarrow r_1 - 2r_2$$ $$\\left[ \\begin{matrix} 1 & 2 \\\\ 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{matrix} \\right. \\right] \\rightarrow \\left[ \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix} \\left| \\, \\begin{matrix} -2 & 1 \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{matrix} \\right. \\right] $$ So the inverse of matrix \\(A\\) is: $$A&#94;{-1} = \\begin{bmatrix} -2 & 1 \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{bmatrix} $$ We can check it by multiplying \\(A&#94;{-1}\\) and \\(A\\) : $$A&#94;{-1}A = \\begin{bmatrix} -2 & 1 \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{bmatrix}\\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}$$ For 3 by 3 or higher dimension matrix, finding inverse can be really tedious. There are other ways to compute the inverse of a matrix, but if you do it by hand, Gaussian elimination is the most efficient one. Now we successfully find the inverse of our matrix, we can finally find the nontrivial solution for our equation \\(A\\pmb{\\color{red}x} = \\pmb{b}\\) for any \\(\\pmb{b}\\) we want. But you might want to ask this question: There are cases that we don't get any solution or get infinite many solution, so if our solution is given by a simple multiplication: $$\\color{red}x = A&#94;{-1} \\pmb{b}$$ Then how can there be infinite many solution or no solution? An easy answer to this question is: For those cases, we couldn't find any inverse. To see that, we have to treat the inverse matrix as a linear transformation. Not all matrices have inverse, for example, any matrix has different numbers of rows and columns cannot have inverse. Let's look at a specific example: $$AB = \\begin{bmatrix} 1 & 2 & 1\\\\ 0 & 1 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix} $$ Apparently, the matrix \\(A\\) multiply by matrix \\(B\\) give you identity matrix, but can you say that the inverse of \\(B\\) is \\(A\\) ? Sort of, but not quite. We can call them left inverse or right inverse . To be a inverse matrix, multiply from left to right or right to left has to returns the same identity matrix. For the above example, we can easily see that: $$BA =\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & 1\\\\ 0 & 1 & 1 \\end{bmatrix} \\neq \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix} $$ How about the matrix that transform 2d space to 1d line? $$ A = \\begin{bmatrix} 2 & -1\\\\ 2 & -1 \\end{bmatrix}$$ Do they have inverse? Again, no. The inverse matrix is like the reverse operation of the previous transformation. If the transformation maps the plane to line, the reverse operation must transform a line to a plane. Which means that for a single vector on the line, it has to transform to many different vector on the plane. That is not a linear transformation can do. It's like functions, for one x value, you can only get one y value. So for one vector input, you can only get one vector output. In practice, we will see that finding the inverse in order to solve linear system is actually not very convenient. Nowadays, computer use other methods like LU decomposition , which is so much faster than finding the inverse. Nevertheless, to have a unique solution, having inverse matrix is a must.","tags":"Chapter 5 Solving Linear system using matrix","url":"5-4-the-cheat-inverse-matrix","loc":"5-4-the-cheat-inverse-matrix"},{"title":"5-5 The fundamental theorem of Linear Algebra","text":"In this chapter, we learned quite a lot of practical concepts like solving linear system or inverse matrix. Now we can finally learn something more abstract and beautiful(in my opinion), which is the fundamental theorem of linear algebra. In here, we are just going to learn the first part of it which also called Rank–Nullity theorem. The name of this theorem is actually not universal. It is popularized by one of my favorite professors: Gilbert Strang . This name is related to the famous theorem in calculus, the fundamental theorem of Calculus. The theorem itself is not hard to prove, so if you are interested, you can try to do it. But before we dig in to the theorem, we will quickly introduce two more concepts. The first one is the Rank of a matrix & transformation. The Rank of a matrix is defined as the dimension of its column space. For example: $$A = \\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix}$$ The Column space is the span of its columns. In this case, we have \\(span \\left( \\begin{bmatrix} 1\\\\ 3 \\end{bmatrix} , \\begin{bmatrix} 2\\\\ 4 \\end{bmatrix} \\right)\\) , the two column vectors are linear independent so they span the entire 2d space. Then the dimension of its column space is 2. So we can say: $$Rank(A) = 2$$ The second one is the Nullity of a matrix & transformation. The Nullity of a matrix is defined as the dimension of its null space or Kernel. For example: $$A = \\begin{bmatrix} 2 & 1\\\\ 2 & 1 \\end{bmatrix}$$ The null space of this transformation can be found by calculate the following equation: $$A\\pmb{x} = \\pmb{0}$$ if you solve this equation, you will find that the solution for \\(\\pmb{x}\\) is \\(span \\left( \\begin{bmatrix} 1\\\\ -2 \\end{bmatrix} \\right)\\) , so the dimension of null space is 1, therefore we can say: $$Nullity(A) = 1$$ Now we finished our appetizer, let's check our main course. And of course, To understand the theorem better, we have to put this in the frame of linear transformation. Let's say we have a transformation matrix \\(T\\) . This matrix transform all vectors in vector space \\(V\\) to another vector space \\(W\\) . So if you pick any vector in \\(W\\) , say \\(\\pmb{w} \\in W\\) . Then their must be at least one vector \\(\\pmb{v}\\) in vector space \\(V\\) , such that: $$\\pmb{w} = T(\\pmb{v})$$ What we are interested is the dimension. So what is the dimension of \\(V\\) ? This is easy. If your matrix \\(T\\) is a \\(m \\times n\\) matrix, it means you take the vector with dimension \\(n\\) and you output a vector with dimension \\(m\\) . So, the input vector space must have dimension of \\(n\\) . How about the dimension of \\(W\\) ? The vector space \\(W\\) has a more complicated situation. You might think, like how we get the dimension of \\(V\\) , the dimension of \\(W\\) is simply \\(m\\) . That is not true. For example: $$A = \\begin{bmatrix} 0 & 0\\\\ 0 & 0 \\end{bmatrix}$$ This zero matrix returns only vector zero, which means our \\(W\\) space has dimension of zero. But \\(m\\) in this case is 2 so not true at all. So what is it? Well, I think I just spoil the answer at the beginning. Yes! You guessed it! It is the Rank of this transformation. Why? Remember how the transformation matrix is defined? The column of the transformation matrix is the basis vectors after transformation. So the span of the \"transformed\" basis has to be the vector space \\(W\\) . There is another name for \\(W\\) . For a linear transformation. Vector space \\(W\\) is called the image of transformation. It is the \"output\" of that transformation. Alright! We now know the dimension of vector space before and after the transformation, there is only one part of the theorem remains. When we looked at the dimension of \\(V\\) and \\(W\\) , you will notice that the dimension of \\(V\\) is always bigger or equal to the dimension of \\(W\\) . This comes from the fact that linear transformation cannot map one vector to two different ones, just like in function, one \\(x\\) value will always give you one \\(y\\) value, so the domain will always bigger than the range. This is not particularly interested, but one might ask another question: where did the \"missing\" dimension go? If your transformation transform a 3d space to a 1d space, the rest 2 dimension actually go to the Null space of that transformation! This is the last piece of our Rank-Nullity theorem: $$Rank(T) + Nullity(T) = dim(V) = n$$ How is this useful in practice? Well, once we know the rank of a transformation, we can quickly tell the dimension of its null space, and this works for unknown matrix too. Even if we don't know the exact transformation in the future, by applying the Rank-Nullity Theorem, we can know the dimension of image space or null space.","tags":"Chapter 5 Solving Linear system using matrix","url":"5-5-the-fundamental-theorem-of-linear-algebra","loc":"5-5-the-fundamental-theorem-of-linear-algebra"},{"title":"5-6 assignments","text":"Solve the linear system by using Gaussian elimination: $$\\begin{bmatrix} 2 & 0 & 1\\\\ 1 & 1 & 0\\\\ 1 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y\\\\ z \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 2\\\\ 3 \\end{bmatrix}$$ Find the inverse of the above matrix by using Gaussian elimination, write down the rank of the matrix and explain why you get & didn't get solutions for question 1. Solve the system without using Gaussian elimination: $$\\begin{bmatrix} 2 & 0 & 1\\\\ 1 & 1 & 0\\\\ 1 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} x\\\\ y\\\\ z \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 3\\\\ 4 \\end{bmatrix}$$","tags":"Chapter 5 Solving Linear system using matrix","url":"5-6-assignments","loc":"5-6-assignments"},{"title":"6-1 Matrix and matrix multiplication","text":"In this Chapter we will work with matrix and matrix multiplication. Of course, just learn how to multiply the matrices together is not going to be the goal of this chapter. But to learn more powerful tools in linear algebra, we need to start from basics. Let's look at two matrix multiplication with vectors \\(A\\pmb{x} = \\pmb{b}\\) : $$\\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}$$ $$\\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} \\color{red}{0} \\\\ \\color{red}{1} \\end{bmatrix} = \\begin{bmatrix} \\color{red}{2} \\\\ \\color{red}{4} \\end{bmatrix}$$ If we combine the vectors into a matrix, and do the same thing with both vector \\(\\pmb{x}\\) and vector \\(\\pmb{b}\\) , we will get : $$\\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 1 & \\color{red}{0} \\\\ 0 & \\color{red}{1} \\end{bmatrix} = \\begin{bmatrix} 1 & \\color{red}{2} \\\\ 3 & \\color{red}{4} \\end{bmatrix}$$ So here you are! This is the way of multiplying matrices! Although we don't know what it is mean or what is it for yet, we can still see something out of it. First you can see that the \"shape\" of matrix is quite important here. Just like when we do transformation, we cannot make a 3 by 3 transformation matrix to transform a 1d vector. To multiply two matrices, the matrices must have certain \"shape\". If we think about matrix & matrix multiplication as the combination of many column vector transformation, then like transformation, the size of input column vector must agree with the size of transformation matrix, to be specific, the column number of the first matrix must be the same as the size of input column vector. Which happens to be the row number of the second matrix. But how about the row number of the first matrix and the column number of the second matrix? Are they useful? Well yes!, they are just happens to be the dimension of the result matrix! The row number of the first matrix became the dimension of output vector and the column number of the second matrix tells you how many of them are combining together to form the final matrix. All of the details above seems complicate and hard to memorize, but here is one good thing about math, you don't have to memorize them at all! Just write down any matrix and start multiplying them and combining the vectors. You will figure out all the details by yourself! Also Luckily, we hardly work with non-square matrices, and for square matrices, rules are pretty simple: \\(n \\times n\\) matrix can only multiply the square matrix with the same size ( \\(n \\times n\\) ) and the result will be another \\(n \\times n\\) matrix.","tags":"Chapter 6 Matrix and Matrix multiplication","url":"6-1-matrix-and-matrix-multiplication","loc":"6-1-matrix-and-matrix-multiplication"},{"title":"6-2  The \"Steps\" in Gaussian elimination","text":"Last time, we learned how to multiply two matrices. In this section, we can finally put this into application. The first application we will do is the elementary matrices. Remember the way we solve the linear system? We use Gaussian elimination. Let's look at a example and see if we can understand the process in a new point of view. Assume we are going to solve the below system: $$ \\left[ \\begin{matrix} 2 & 4 \\\\ 1 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1 \\end{matrix} \\right. \\right] $$ The way we solve it is to do some row operation, namely you take one row in the matrix and replace it with a modified version of it or you swap two rows. There are two ways to modify the rows, which is adding and multiplying. Let's take a look at one row operation. $$ \\color{red}{(1)}\\rightarrow \\frac{1}{2} \\times\\color{red}{(1)} $$ $$\\begin{bmatrix} 2 & 4\\\\ 1 & 1 \\end{bmatrix}\\rightarrow\\begin{bmatrix} 1 & 2\\\\ 1 & 1 \\end{bmatrix}$$ Now think about this: We learned the way of multiplying matrix and vector, the multiplication transformed that vector to another vector. So, can we write the above transformation as a matrix & matrix multiplication? Let's look at the columns before and after transform: $$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\rightarrow\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$ $$\\begin{bmatrix} 4\\\\ 1 \\end{bmatrix}\\rightarrow\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}$$ The \"transformation\" only made x value to be a half and leaved the y value, so if we are going to write the \"transformation\", it should be: $$E_1 = \\begin{bmatrix} \\frac{1}{2} & 0\\\\ 0 & 1 \\end{bmatrix}$$ We can check it by multiplying the two matrices: $$\\begin{bmatrix} \\frac{1}{2} & 0\\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} 2 & 4\\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}\\times2 & \\frac{1}{2}\\times4\\\\ 1\\times1 & 1\\times1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2\\\\ 1 & 1 \\end{bmatrix}$$ Great, now we know that we can describe the row operation as a specific matrix multiply on it. Actually, their is a name for that type of matrix, it is called elementary matrix . We can try to write down more elementary matrix in 2d and see what they looks like. Say we want to add the second row to the first row and replace the first one, what should we do? We first start with an identity matrix. For 2d, the matrix will be a \\(2\\times2\\) matrix. $$I_{2\\times2}= \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}$$ Now let's replace the first row: $$E_2 =\\begin{bmatrix} 1+0 & 0+1\\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1\\\\ 0 & 1 \\end{bmatrix}$$ Similarly, we can describe the swapping using elementary matrix: $$E_3 = \\begin{bmatrix} 0 & 1\\\\ 1 & 0 \\end{bmatrix} $$ For now, elementary matrix is just a fancy way to write down the row operation of matrices. But why do we need such way to describe row operation? Let's look at one of the application of row operation, the Gaussian elimination. Remember we solve linear system \\(A\\pmb{x}=\\pmb{b}\\) using the Gaussian elimination? What if we rewrite all the row operation to elementary matrices? We start with writing the augmented matrix: $$A|\\pmb{b}= \\left[ \\begin{matrix} 2 & -1 \\\\ 1 & 1 \\end{matrix} \\left| \\, \\begin{matrix} 2 \\\\ 1 \\end{matrix} \\right. \\right] $$ Then we did some row operations to make the matrix to be the identity matrix: $$A\\rightarrow E_nE_{n-1}...E_2E_1A = I$$ At the same time, we do the same operations onn the vector \\(\\pmb{b}\\) : $$\\pmb{b}\\rightarrow E_nE_{n-1}...E_2E_1\\pmb{b}$$ If you take all the elementary matrice and multiply all of them together, we get: $$E_nE_{n-1}...E_2E_1A = E_{total}A= I$$ Notice the definition of a matrix's inverse is: $$A&#94;{-1}A = AA&#94;{-1}= I$$ So, the matrix \\(E_{total}\\) is exactly the inverse matrix of \\(A\\) ! Once we know that, we can replace the elementary matrices into \\(A&#94;{-1}\\) : $$E_nE_{n-1}...E_2E_1\\pmb{b} = E_{total}\\pmb{b} = A&#94;{-1}\\pmb{b} = \\pmb{x}$$ Well, this is exactly the method we used in the inverse matrix unit. Great! Let's see if we can do more. The second time that we used the row operation is to find the inverse of a matrix. Back then, we didn't explain why the method work, now we learned the elementary matrices, we can see what actually happens. To find the inverse, we first place the matrix and the identity matrix with the same size together: $$A|I = \\left[ \\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix} \\left| \\, \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix} \\right. \\right] $$ Then we do Gaussian elimination for both side. The goal is to make the left side of augmented matrix to be identity matrix. $$A \\rightarrow I $$ We can write the row operation as multiple elementary matrices: $$E_nE_{n-1}...E_2E_1A = I$$ Again, it is obvious that \\(E_nE_{n-1}...E_2E_1 =A&#94;{-1}\\) so if we do the same row operations on the right side of augmented matrix(the identity matrix), we will get: $$ I \\rightarrow E_nE_{n-1}...E_2E_1I = A&#94;{-1}I = A&#94;{-1}$$ That is why we end up with the inverse of matrix \\(A\\) .","tags":"Chapter 6 Matrix and Matrix multiplication","url":"6-2-the-steps-in-gaussian-elimination","loc":"6-2-the-steps-in-gaussian-elimination"},{"title":"6-3  Row Reduced Echelon Form","text":"We already saw several interesting thing we can do with Matrix and Matrix multiplication, in this section, we will learn a very famous matrix. This matrix is always introduced at the first class of linear algebra, but here we are going to try a very special approach. Assume we have a lovely matrix \\(A\\) , where \\(A\\) has some interesting columns: $$A = \\begin{bmatrix} 2 & -1 & 1\\\\ 1 & 1 & 2\\\\ 1 & 2 & 3 \\end{bmatrix}$$ The columns of matrix \\(A\\) has some relations, namely the first column plus the second column gives you the third column: $$\\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}+\\begin{bmatrix} -1\\\\ 1\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 2\\\\ 3 \\end{bmatrix}$$ What do we know about the columns of this matrix? We know that the real \"meat\" of the matrix is the first two columns, since the last column is the linear combination of the first two. So, how about we deconstruct this matrix into two? where the first matrix contain all the \"meat\" of the matrix (here the \"meat\" just means the basis of column space), and the second matrix tells us all the other information, like the last column is the first column plus the second column? Let's do this! first matrix is easy! We just get rid of the last column and call it \\(C\\) : $$C = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$$ then we write the \"bone\" of this matrix using the basis \\(C\\) . How do we construct it? We can immediately tell that to get the first column \\( \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix} \\) , we need just one portion of \\( \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix} \\) and zero portion of \\( \\begin{bmatrix} -1\\\\ 1\\\\ 2 \\end{bmatrix} \\) . In another word, one portion of the first basis vector and zero of the second: $$R_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$$ To get the second column vector, we need zero portion of \\( \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix} \\) and one portion of \\( \\begin{bmatrix} -1\\\\ 1\\\\ 2 \\end{bmatrix} \\) . In another word, one portion of the first basis vector and zero of the second: $$R_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$ At last, To get the third column vector, we need one portion of \\( \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix} \\) and one portion of \\( \\begin{bmatrix} -1\\\\ 1\\\\ 2 \\end{bmatrix} \\) . In another word, one portion of the first basis vector and zero of the second: $$R_3 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$ Now we put these three \"bones\" together, and we call the combination matrix \\(R\\) : $$R = \\begin{bmatrix} 1 & 0 & 1\\\\ 0 & 1 & 1 \\end{bmatrix}$$ You can check that the matrix \\(C\\) multiply by the matrix \\(R\\) is exactly the matrix \\(A\\) ! $$A = \\begin{bmatrix} 2 & -1 & 1\\\\ 1 & 1 & 2\\\\ 1 & 2 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 1\\\\ 0 & 1 & 1 \\end{bmatrix} = CR$$ Here comes the cool part. Do you know why we name the \"bone\" matrix \\(R\\) ? The \\(R\\) stand for row, if you learned the Gaussian elimination before, you will notice that this matrix looks like the result of the Gaussian elimination, the Row Echelon Form ! Specifically, in this case, we get the Reduced Row Echelon Form of matrix \\(A\\) . We can do this for any matrix. When you do it on the square matrix with all independent columns(rows), the Reduced Row Echelon Form( RREF ) is just the identity matrix. So, not only we can combine two matrix into one, we can also decompose a matrix into many. This specific example is called CR factorization, there are more of them in the future, like LU decomposition or diagonalization. Each of them plays an important role in linear algebra.","tags":"Chapter 6 Matrix and Matrix multiplication","url":"6-3-row-reduced-echelon-form","loc":"6-3-row-reduced-echelon-form"},{"title":"6-4  Change the basis","text":"Now comes to one of the biggest applications of Matrix multiplication. Sometimes we we solve a problem, we might find that the classic Cartesian coordinate is not the best coordinate to work with. For example, let start with some random point on the Cartesian coordinate and form a line segment: Say we want to find the tip of the bar after a rotation of 15 degree, to do this normally takes quite a lot of work. But, if we convert the point \\((3,1)\\) to the polar coordinate, and then we do the rotation. After that, we can covert the transformed polar coordinate into Cartesian coordinate. So, can we do the same in linear algebra? Can we change one vector in one coordinate(basis) to another one? Let's start with a vector in the classic Cartesian coordinate, pick any two number, let's say \\((2,1)\\) . The basis of Cartesian coordinate is vector \\( \\begin{bmatrix}1 \\\\ 0 \\end{bmatrix} \\) and \\( \\begin{bmatrix} 0\\\\ 1\\end{bmatrix} \\) , so the meaning of vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) is just two pieces of \\( \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} \\) and one piece of \\( \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} \\) : $$\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} = 2\\times\\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} + 1\\times\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix}$$ Here we have another 2d basis create by spanning vector \\( \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\) and \\( \\begin{bmatrix} -1\\\\ 2 \\end{bmatrix} \\) : In this new basis, our vector \\( \\begin{bmatrix} \\color{red}2\\\\ \\color{red}1 \\end{bmatrix} \\) will no longer looks the same as the vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) in Cartesian coordinate. We can do a quick calculation to see what is \\( \\begin{bmatrix} \\color{red}2\\\\ \\color{red}1 \\end{bmatrix} \\) in the new basis: $$\\pmb{\\color{red}v} = 2\\times\\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} + 1\\times\\begin{bmatrix} 1\\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 3\\\\ 1 \\end{bmatrix}$$ To not make you confuse, I will label all the vectors lie in Cartesian coordinate black, and the vector lie in new coordinate red. So the question is, what should be the vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) in our new basis? In another word, how many \\( \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\) and \\( \\begin{bmatrix} -1\\\\ 2 \\end{bmatrix} \\) combined together will give you \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) ? You might find the statements here are kind of confusing, so I will help you out by showing you the solution first: It turns out, if you take \\(\\color{red}{\\frac{5}{3}}\\) of vector \\( \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\) and \\(\\color{red}{\\frac{1}{3}}\\) of vector \\( \\begin{bmatrix} 1\\\\ -2 \\end{bmatrix} \\) , you will end up with vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) . So our vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) in the new basis has a new name \\( \\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} \\) . Now how do I got these number out? Think about this, if we rewrite the above relation in a matrix form, we will have: $$\\color{red}{\\frac{5}{3}}\\times\\begin{bmatrix} 1\\\\ 1 \\end{bmatrix}+\\color{red}{\\frac{1}{3}}\\times\\begin{bmatrix} 1\\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}\\rightarrow\\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix}\\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}$$ It looks like the matrix \\( \\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix} \\) can transformed our \\( \\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} \\) back to what it was in Cartesian coordinate. What do we want is to know what is \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) in new basis, so we have to find the inverse: $$\\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix}\\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix}&#94;{-1}\\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}= \\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} $$ Strange, right? To know the presentation in one specific basis, you have to apply the inverse transformation. but this is what it is. For the previous example, the way we get \\( \\begin{bmatrix} \\color{red}{\\frac{5}{3}}\\\\ \\color{red}{\\frac{1}{3}} \\end{bmatrix} $ is by first put our basis vector together and form the matrix $ \\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix} \\) , then we can find its inverse: $$\\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix}&#94;{-1} = \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3}\\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix}$$ And the last step is to put any vector we want to transform in it and do a multiplication. Can we do it for any basis? Here is the animation I made that shows the vector \\( \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} \\) in different basis: Now we know how to convert a vector in another basis, let's do something fun here. Assume you have some transformation \\(T\\) in some basis. For example, the Cartesian coordinate. The transformation \\(T\\) has a dimension of \\(n \\times n\\) . Let's say we have a vector lies in the Cartesian coordinate called \\(\\pmb{a}\\) and if you transform that vector, you will get vector \\(\\pmb{b}\\) : $$T\\pmb{a} = \\pmb{b}$$ Here comes the cool part. Instead of doing this transform in the old, boring Cartesian coordinate, why not do it in a new fancy basis? The basis vectors of this new basis form a matrix \\(P\\) . We can rewrite the vector \\(\\pmb{a}\\) and \\(\\pmb{b}\\) in our new basis: $$\\pmb{a}' = P&#94;{-1}\\pmb{a}$$ $$\\pmb{b}' = P&#94;{-1}\\pmb{b}$$ We know the fact that you can use the transformation \\(T\\) to transform \\(\\pmb{a}\\) to \\(\\pmb{b}\\) but you can see that wouldn't work in our new basis. Namely, you cannot apply the same transformation \\(T\\) on \\(\\pmb{a}'\\) to give you \\(\\pmb{b}'\\) : $$\\pmb{a}' = P&#94;{-1}\\pmb{a}$$ $$T\\pmb{a}' = TP&#94;{-1}\\pmb{a} \\neq \\pmb{b}'$$ So we need a new \"look\" of our transformation \\(T\\) in the new basis. We shall name it \\(T'\\) , so that: $$T'\\pmb{a}' = \\pmb{b}'$$ With some simple algebra, we can get the expression of \\(T'\\) : $$T'\\pmb{a}' = \\pmb{b}'$$ $$T'\\pmb{a}' = P&#94;{-1}\\pmb{b}$$ $$T'\\pmb{a}' = P&#94;{-1}T\\pmb{a}$$ $$T'\\pmb{a}' = P&#94;{-1}TP\\pmb{a}'$$ $$T' = P&#94;{-1}TP$$ We can see that the way we rewrite a transformation for new basis is kind of different than vector. To understand the expression \\(T' = P&#94;{-1}TP $ is not hard. Imagine you are doing business in the other country. The transformation is the business process(Take some money and return some money). $T'\\) and \\(T\\) is just the same business process but in different country. say one in US one in UK . The matrix \\(P\\) convert one currency to another, in our case dollar to pound. So if you are American and want to do the business in US , all you need is the transformation \\(T\\) . But if you are a gentleman from UK , The same business process became to: 1) Convert the money to dollar ( \\(P\\) ) 2) Do the business in US ( \\(T\\) ) 3) Convert the dollar you earned in US back to pound and enjoy your life ( \\(P&#94;{-1}\\) ) So overall, what you did is: $$\\pmb{a} \\rightarrow P\\pmb{a} \\rightarrow TP\\pmb{a} \\rightarrow P&#94;{-1}TP\\pmb{a}$$ and that is the process \\(T'\\) . Notice that what we are dealing with is the \\(n \\times n\\) case. We can generalized this idea to any dimensions, but that is not what we focus here. Most of the time, we will be working on \\(n \\times n\\) case, especially \\(3 \\times 3\\) or \\(4 \\times 4\\) for physics.","tags":"Chapter 6 Matrix and Matrix multiplication","url":"6-4-change-the-basis","loc":"6-4-change-the-basis"},{"title":"7-1  Eigenvalue and Eigenvector","text":"So far, we have been working on many matrices. We also learned that the cool notation \"matrix\" can describe a lot of things: linear equations, transformation, rotation, change the basis… We can rely on matrix to do a lot of calculation that couldn't be done or hard to describe before. However, in real life, even if we use the matrix to cheat, the questions can still be really hard and tedious. In this chapter, we will be focusing on ONE way to make our life easier. Also, it turns out, this specific method has a huge impact on many many area. Our main character has a German name: Eigen . It is not because the guy who invent this is a German, but because David Hilbert, a German mathematician gave him this classic German name. From then, everyone start calling him this. Although this idea is first arose in the study of quadratic forms and differential equations, we are not going to take that approach. Instead, let's first define what it is, and then see what can this bad boy do. So let's look at an example matrix: $$A = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}$$ If we treat this matrix as a transformation, applying this transformation on any 2d vector will give you another 2d vector. But this is not a new story, we did this before a billion times. What is interesting is that for some vectors in 2d space, their direction will not change after the transformation. What does that mean algebraically? Assume for some vector \\(\\pmb{x}\\) , we have the relation: $$A\\pmb{x} = \\lambda\\pmb{x}$$ Where \\(\\lambda\\) is some non zero scalar value. Notice this value is just tells us how the vector is been \"stretched\" or \"compressed\". If the \\(\\lambda\\) turns out to be negative, it just means the direction is reversed. Let's look at one of such vector. In our case, if we input vector \\( \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\) and calculate the vector after the transformation, we get: $$A\\pmb{x} = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}\\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix}$$ What a coincident! If we multiply 2 on both side, we get: $$2\\times A\\pmb{x} = 2\\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}\\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}\\begin{bmatrix} -2\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2\\\\ 2 \\end{bmatrix}$$ What does the above equation means? It means not only the vector \\( \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\) can do that, but any multiple of vector \\( \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\) also satisfy the equation \\(A\\pmb{x} = \\lambda\\pmb{x}\\) ! For this specific vector, our vector stay the same after the transformation. So, \\(\\lambda\\) in this case is just \\(1\\) . If we can find such vector and the corresponding \\(\\lambda\\) that satisfy the equation \\(A\\pmb{x} = \\lambda\\pmb{x}\\) . Then we call the vector \\(\\pmb{x}\\) the Eigenvector of our matrix \\(A\\) , and we call the value \\(\\lambda\\) the corresponding Eigenvalue of the matrix. Great, now we know what does that means algebraically. Can we visualize this? If you take the transformation and graph the change on different vector input, you will get something looks like this: As we expected, the vector \\( \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\) and all the vectors along that line stays on that line after the transformation. But if you pay more attention, along the y-axis, the vectors do not change their direction too. We can check it by multiplying any vector along y-axis: $$A\\pmb{x}' = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}\\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} =\\begin{bmatrix} 0\\\\ 2 \\end{bmatrix}$$ So any vector on y-axis will get doubled after the transformation. What does that means? Well, that vector didn't change the direction(still pointing along y-axis) so it is obviously another eigenvector of our matrix \\(A\\) . What is the eigenvalue corresponding to that eigenvector? Since the vectors get doubled, then the eigenvalue must be two. Wonderful! We have two different eigenvectors and eigenvalues for a single matrix. Can we find more? Unfortunately, we can't in this case. Later on, we will know why that is the case. Now, you might wondering, does that means the eigenvectors and eigenvalues exist for any matrices? If so, how could we find them without guessing? We will leave that question to the future chapter.","tags":"Chapter 7 Eigenvector, eigenvalue and Diagonalization","url":"7-1-eigenvalue-and-eigenvector","loc":"7-1-eigenvalue-and-eigenvector"},{"title":"7-2  Characteristic polynomials","text":"Last time, we learned a special vector for linear transformation: Eigenvector. But, there are two questions we didn't answer. One, Is there always eigenvector and eigenvalues for matrices? Second, How do we calculate them? Let's start from the definition of eigenvector and eigenvalue: $$A\\pmb{x} = \\lambda\\pmb{x}$$ If this equation hold for some vector \\(\\pmb{x}\\) and non-zero value \\(\\lambda\\) , then great! We found the eigenvector and eigenvalues for that matrix. To solve this, let's try to move all the terms on one side: $$A\\pmb{x} - \\lambda\\pmb{x} = 0$$ Notice that we only know the matrix \\(A\\) , so we need some way to combine these two terms. Factoring out vector \\(\\pmb{x}\\) seems to be a good idea, but is there any way to factor out the vector \\(\\pmb{x}\\) ? Yes! We know the fact that for the square matrix with the same size: $$(A+B)\\pmb{x} = A\\pmb{x}+B\\pmb{x}$$ So if we can rewrite number \\(\\lambda\\) into a matrix with the same size as \\(A\\) , then we can combine the matrices and factor out \\(\\pmb{x}\\) . But how do we rewrite number to a matrix? That is not hard since this number is multiplying on a vector, We can add a linear transformation that does nothing in front of vector \\(\\pmb{x}\\) : $$\\lambda\\pmb{x} = \\lambda I\\pmb{x} = (\\lambda I)\\pmb{x} $$ Now, the scalar \\(\\lambda\\) is a linear transformation \\(\\lambda I\\) , therefore, we can add the matrix \\(A\\) and the new matrix \\(\\lambda I\\) : $$A\\pmb{x} - \\lambda I\\pmb{x} = 0$$ $$(A- \\lambda I)\\pmb{x} = 0$$ The new matrix \\((A- \\lambda I)\\) multiply some vector \\(\\pmb{x}\\) gives you zero vector. How do we find the vector \\(\\pmb{x}\\) ? Easy! This is just ask you to find the null space of matrix \\((A- \\lambda I)\\) ! Before, to find null space, we need to solve the linear system \\(A\\pmb{x} = 0\\) . But in our case, the matrix itself has some unknown variable in it, which is the number \\(\\lambda\\) . So how could we find the null space of a unknown matrix? Here we will introduce a technique that we didn't mention before, call determinant . In short, the determinant calculate the ratio between the size of space before and after the linear transformation. In 2d, it is simply the ratio between the area before and after the linear transformation. To calculate determinant for any \\(2\\times2\\) matrix, we just need to do: $$det(A) = \\begin{vmatrix} a & b\\\\ c & d \\\\ \\end{vmatrix} = ad - bc$$ We are not going to cover the method of finding \\(3\\times3\\) or higher dimension matrix's determinant in this chapter. Feel free to search them up and think about why. Why is find the ratio of area useful in our case? Because if a non-zero null space exist for a matrix, the the determinant of this matrix must be zero. Why? By the rank-nullity theorem, if null space has dimension greater than zero, it means our rank is less than the dimension of matrix. So the linear transformation will transform the space to a space with lower dimension. If the matrix is \\(2\\times2\\) but the rank is lower than 2, for example, is 1. It just means the matrix will transform any 2d vector to a 2d vector on a line (1d). So what will be the ratio of area before and after the transformation? Obviously is zero(The whole 2d plane was \"smash\" into a line). Great! Now is time to put everything together and calculate some eigenvalue & eigenvector! Let's find the eigenvalue & eigenvector for the matrix we discussed last time: $$A = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}$$ The first thing we do is to get the matrix \\(A - \\lambda I\\) : $$A - \\lambda I = \\begin{bmatrix} 1 & 0\\\\ 1 & 2 \\\\ \\end{bmatrix}-\\lambda \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1-\\lambda & 0\\\\ 1 & 2-\\lambda \\\\ \\end{bmatrix}$$ Then, we write down the determinant of this matrix: $$\\begin{vmatrix} 1-\\lambda & 0\\\\ 1 & 2-\\lambda \\\\ \\end{vmatrix} = (1-\\lambda )(2-\\lambda) - 0\\times1 = (1-\\lambda )(2-\\lambda)$$ To solve \\(\\lambda\\) , we just need to set this expression to zero. But before we do that, look at this familiar expression! It is a quadratic! Later, you will see that the determinant of \\(A - \\lambda I\\) will always be a polynomial. We have a special name for this: the Characteristic polynomials . In our case, this polynomials is extremely easy to solve: $$(1-\\lambda )(2-\\lambda) = 0 $$ $$\\downarrow$$ $$\\lambda_1 = 1$$ $$\\lambda_2 = 2$$ So, \\(\\lambda_1 = 1\\) and \\(\\lambda_2 = 2\\) are the eigenvalue of the matrix \\(A\\) . To solve the eigenvector correspond to these eigenvalue, we just plug these back into our matrix \\(A - \\lambda I\\) and find the null space: $$A - \\lambda_1 I = \\begin{bmatrix} 1 -1& 0\\\\ 1 & 2 -1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 & 0\\\\ 1 & 1 \\\\ \\end{bmatrix} $$ To find the null space, we solve the linear system \\(A\\pmb{x} = 0\\) : $$\\begin{bmatrix} 0 & 0\\\\ 1 & 1 \\\\ \\end{bmatrix}\\begin{bmatrix} x\\\\ y \\end{bmatrix}= 0$$ $$\\downarrow$$ $$ \\begin{cases} 0x+0y\\ =\\ 0 \\\\ x+y\\ =\\ 0 \\end{cases} \\rightarrow x = -y \\rightarrow span{\\begin{bmatrix} -1\\\\ 1 \\end{bmatrix}} $$ Therefore, the eigenvector correspond to eigenvalue \\(1\\) is vector \\( \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\) . Using the same method, we can get the eigenvector for eigenvalue \\(\\lambda_2 = 2\\) is vector \\( \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} \\) . Awesome! We solved all the eigenvalue and their eigenvector. Does that means we solved everything? Of course not. Remember, we got lucky when solving the polynomial. Let's go back to the characteristic polynomial and see what are we missing. The characteristic polynomial we had is \\((1-\\lambda )(2-\\lambda)\\) , but in principle, we could have any polynomials. Take \\(2\\times2\\) matrix as example, our characteristic polynomial will be a quadratic: $$a\\lambda&#94;2 + b\\lambda + c= 0$$ For our case, we ended up with two distinct \\(\\lambda\\) values. But we could also have two same \\(\\lambda\\) values or no real \\(\\lambda\\) . The two same \\(\\lambda\\) values case is not that interesting. In 2d, the transformation that has only one eigenvalue will looks like a shear transformation. The interesting case is when we have no real \\(\\lambda\\) values. Let's imaging: What kind of transformation in 2d that has no eigenvector? Having eigenvectors means there are some directions where all the vector alone that direction do not change its direction after the transformation. So, having no eigenvector means all the vector in 2d change its direction after the transformation. Did we see any of the transformation that can do this? Of course! Any transformation that rotate the plane! Let's take the very first linear transformation we saw. As you can see, all the vector do not end up with the same direction after the transformation. If you learned the imaginary number before(don't worry if you never learn it, but I highly recommend you check it out online if you never), you will know that the quadratic that has no real solution will have imaginary solution. So our eigenvalue will be imaginary. In fact, imaginary eigenvalue will always has something to do with the rotation. However, Imaginary eigenvalue is beyond the scope of our note.","tags":"Chapter 7 Eigenvector, eigenvalue and Diagonalization","url":"7-2-characteristic-polynomials","loc":"7-2-characteristic-polynomials"},{"title":"7-3  Diagonal matrix and transpose","text":"From the last chapter, we learned the way to compute the eigenvectors and eigenvalues of a matrix. We also learned the geometric meaning of eigenvectors and eigenvalues. You might say: Sure, here is the eigenvectors and eigenvalues of a matrix. So what? What is the application? Don't worry! You will find out the good stuff real soon. But for now, we have to learn one interesting type of matrix, which is related to the \"good stuff\" later. The special matrix is called the diagonal matrix. The name \"diagonal\" comes from the fact that the matrix only has non-zero entry on the \"diagonal\" of the square matrix: $$ A = \\begin{bmatrix} x_{11} & 0 & 0 & \\dots & 0 \\\\ 0 & x_{22} & 0 & \\dots & 0 \\\\ 0 & 0 & x_{33} & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\dots & x_{nn} \\end{bmatrix} $$ In convention, we call the the entries that has the same column and row number \"diagonal\" terms. and the entries that has the different column and row number \"off-diagonal\" terms. For diagonal matrix, First, it is usually a \\(n\\times n\\) matrix (we don't consider the \"rectangular\" ones\"). Second, all the \"off-diagonal\" terms must be zero. Notice that the diagonal terms doesn't have to be zero. Great, this matrix looks VERY simple! Indeed, it is very easy to work with. Let's look at an interesting example: Let's say there is a matrix \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} $, then what is $A&#94;2\\) ? We can do a quick matrix multiplication to get the answer for that: $$A&#94;2 = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 1 + 6 & 2 + 8 \\\\ 3 + 12 & 6 + 16 \\end{bmatrix} = \\begin{bmatrix} 7 & 10 \\\\ 15 & 22 \\end{bmatrix}$$ Well that was a breeze. But what if someone ask you, what is \\(A&#94;{201}\\) ? Then we are screwed. However, if you ask the same question on a diagonal matrix \\(D\\) : $$ D = \\begin{bmatrix} 1 & 0\\\\ 0 & 2 \\end{bmatrix} $$ the result of \\(D&#94;{201}\\) is simply: $$ D&#94;{201} = \\begin{bmatrix} 1&#94;{201} & 0\\\\ 0 & 2&#94;{201} \\end{bmatrix} $$ Not only that, if you tried to find the eigenvalue of a diagonal matrix, you will soon realized the characteristic polynomials of a diagonal matrix is extremely simple. If take matrix \\(D = \\begin{bmatrix} 1 & 0\\\\ 0 & 2 \\end{bmatrix} \\) as a example. The characteristic polynomials is just: $$det(D-\\lambda I) = det(\\begin{bmatrix} 1-\\lambda & 0\\\\ 0 & 2-\\lambda \\end{bmatrix}) = (1-\\lambda)(2-\\lambda) = 0$$ This tells us the eigenvalue right away, \\(\\lambda = 1\\) or \\(\\lambda = 2\\) . Which happens to be the diagonal terms of the matrix. Also, the eigenvectors of the matrix are easy too(Try to find them by yourself). Alright! We now learned everything we need for the grand finale. In the next section, we are going to combine all the pieces and construct one of the most powerful weapons in linear algebra: Diagonalization.","tags":"Chapter 7 Eigenvector, eigenvalue and Diagonalization","url":"7-3-diagonal-matrix-and-transpose","loc":"7-3-diagonal-matrix-and-transpose"},{"title":"7-4 Diagonalization","text":"Last time, we learned a special matrix called diagonal matrix. We knew that if a matrix is a diagonal matrix, everything will be extremely easy to solve.","tags":"Chapter 7 Eigenvector, eigenvalue and Diagonalization","url":"7-4-diagonalization","loc":"7-4-diagonalization"}]};